{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c97f4dfbfe34958adc6618c3b24b19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ad38378ad7a450ebf09a986909d5305",
              "IPY_MODEL_d499a2a70c834dada215f0105355150c",
              "IPY_MODEL_f01b1fc27a984827acb30ebf02cb0fa1",
              "IPY_MODEL_fee02776802e46d5a545d3845393585f",
              "IPY_MODEL_00f06e5b6b74407a88767375884fbf63"
            ],
            "layout": "IPY_MODEL_b52bcf840e4445a191f2e8ee0b5789d7"
          }
        },
        "9ad38378ad7a450ebf09a986909d5305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79702cb4dd5b400e98073b2e3a64556e",
            "placeholder": "​",
            "style": "IPY_MODEL_aa19ed252eda40ccbebb1c268620013a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d499a2a70c834dada215f0105355150c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_97794bf16f0141f39c63098c79945e84",
            "placeholder": "​",
            "style": "IPY_MODEL_3b4fc0778cf7436386907d177d718be8",
            "value": ""
          }
        },
        "f01b1fc27a984827acb30ebf02cb0fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_63f4a9320cb5460c9141cf6f7f6c1279",
            "style": "IPY_MODEL_7e4226b5361f43bcb803a4bdd42e3930",
            "value": true
          }
        },
        "fee02776802e46d5a545d3845393585f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d731868eec474bd5a42edff18c990085",
            "style": "IPY_MODEL_a8f7a943c65e499ba2312a1ede7e3968",
            "tooltip": ""
          }
        },
        "00f06e5b6b74407a88767375884fbf63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445381174d9841e6933d5f24cb686863",
            "placeholder": "​",
            "style": "IPY_MODEL_42cae858a1ac4983a25b76aef86a4960",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b52bcf840e4445a191f2e8ee0b5789d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "79702cb4dd5b400e98073b2e3a64556e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa19ed252eda40ccbebb1c268620013a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97794bf16f0141f39c63098c79945e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4fc0778cf7436386907d177d718be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63f4a9320cb5460c9141cf6f7f6c1279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4226b5361f43bcb803a4bdd42e3930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d731868eec474bd5a42edff18c990085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f7a943c65e499ba2312a1ede7e3968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "445381174d9841e6933d5f24cb686863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42cae858a1ac4983a25b76aef86a4960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df4eb3e94e1141f3b5c6e5d3e17cfa89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_956b9bd4bb754048bae343eea6c03e6d",
              "IPY_MODEL_995fe5f6257645c89ad8af4f0666c47c",
              "IPY_MODEL_bddfea4a4c934c4bb6120f849fc3edb7"
            ],
            "layout": "IPY_MODEL_0b721e2b559a42e0b12f13b1a539ba14"
          }
        },
        "956b9bd4bb754048bae343eea6c03e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7609836970764855bf1f4bc23c16e592",
            "placeholder": "​",
            "style": "IPY_MODEL_84b2f030cfd34371b4d99cd66da68a26",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "995fe5f6257645c89ad8af4f0666c47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c7abd404a864b138a986920d255218a",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_766c4a1a20f8473da9d7d74712553ec6",
            "value": 625
          }
        },
        "bddfea4a4c934c4bb6120f849fc3edb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_099c61f6b83945a2afa37bbce4f7e699",
            "placeholder": "​",
            "style": "IPY_MODEL_2371b48b9ca14b1fad6a24518a1bd9ae",
            "value": " 625/625 [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "0b721e2b559a42e0b12f13b1a539ba14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7609836970764855bf1f4bc23c16e592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b2f030cfd34371b4d99cd66da68a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c7abd404a864b138a986920d255218a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "766c4a1a20f8473da9d7d74712553ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "099c61f6b83945a2afa37bbce4f7e699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2371b48b9ca14b1fad6a24518a1bd9ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d915fa5fb1d5432981326c5c43c8e0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549aff90cb1c42dc96bae0dd32e32bfc",
              "IPY_MODEL_c4286162d8dc4f829f322607958f1c42",
              "IPY_MODEL_6f6e720fa9e84e38a22b3b374b0f3936"
            ],
            "layout": "IPY_MODEL_f10bf43e45dd42a094882ab45ed118c4"
          }
        },
        "549aff90cb1c42dc96bae0dd32e32bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30cfc81d7f92481bbbcbc2ca66deb9da",
            "placeholder": "​",
            "style": "IPY_MODEL_c15ee309f09e419eae514221e083672a",
            "value": "Downloading (…)model.bin.index.json: 100%"
          }
        },
        "c4286162d8dc4f829f322607958f1c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b82acde8382f4cbd8061c22066c6f3ad",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_763d7517ff31481390076fbd1bb5d791",
            "value": 26788
          }
        },
        "6f6e720fa9e84e38a22b3b374b0f3936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df9d3674ccc459799f57303815a91f2",
            "placeholder": "​",
            "style": "IPY_MODEL_78cca83ded014b9383b98f7ac5d04076",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.32MB/s]"
          }
        },
        "f10bf43e45dd42a094882ab45ed118c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30cfc81d7f92481bbbcbc2ca66deb9da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c15ee309f09e419eae514221e083672a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b82acde8382f4cbd8061c22066c6f3ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763d7517ff31481390076fbd1bb5d791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6df9d3674ccc459799f57303815a91f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78cca83ded014b9383b98f7ac5d04076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa7077e06a3e4880b23a17f16684855e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0174e9980b1a49ea86aea6884a712bae",
              "IPY_MODEL_7c5054403e0b4f51b9c34b8db50434d3",
              "IPY_MODEL_9766ec66fcc7457aa0f90bf5c965a87f"
            ],
            "layout": "IPY_MODEL_eb755ecaa63f42d2a5e2376bbb3282fc"
          }
        },
        "0174e9980b1a49ea86aea6884a712bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e06e8fb16cb4e2cbec03189992bf800",
            "placeholder": "​",
            "style": "IPY_MODEL_4fde35aef01547878bd91ff9fee11faf",
            "value": "Downloading shards: 100%"
          }
        },
        "7c5054403e0b4f51b9c34b8db50434d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02ae7365a284d1fbe8ab4e4db190943",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d99c9c60bc064898980722a23471dc36",
            "value": 10
          }
        },
        "9766ec66fcc7457aa0f90bf5c965a87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98dd63d181734b75a89ad1006673f4e2",
            "placeholder": "​",
            "style": "IPY_MODEL_39bd5ae287254be3966c88e37a895b61",
            "value": " 10/10 [06:01&lt;00:00, 18.70s/it]"
          }
        },
        "eb755ecaa63f42d2a5e2376bbb3282fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e06e8fb16cb4e2cbec03189992bf800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fde35aef01547878bd91ff9fee11faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d02ae7365a284d1fbe8ab4e4db190943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d99c9c60bc064898980722a23471dc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98dd63d181734b75a89ad1006673f4e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bd5ae287254be3966c88e37a895b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0911e61804ec42c89876a686f6920b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94e10736a7924b1190d303408ca1151a",
              "IPY_MODEL_0e5bacf1ae524b87bb1b867d5ac0bd84",
              "IPY_MODEL_4e9fbd8b824246778d6a8090d99a743e"
            ],
            "layout": "IPY_MODEL_8b5dcd5e73144a1099c47708a8db195d"
          }
        },
        "94e10736a7924b1190d303408ca1151a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f579070eab0426a98b58d2b04525b77",
            "placeholder": "​",
            "style": "IPY_MODEL_74049dfd556e4358a7baaa5621b419d3",
            "value": "Downloading (…)l-00001-of-00010.bin: 100%"
          }
        },
        "0e5bacf1ae524b87bb1b867d5ac0bd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89392e4ac28d4a3f90d9a1227e505de1",
            "max": 2952899389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a40f29c89a164710b90a54e18e1b76ce",
            "value": 2952899389
          }
        },
        "4e9fbd8b824246778d6a8090d99a743e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601789241e4f4f4b989dafadfc7dde19",
            "placeholder": "​",
            "style": "IPY_MODEL_190e66177b384643ac35ff794e3fb0e5",
            "value": " 2.95G/2.95G [00:15&lt;00:00, 309MB/s]"
          }
        },
        "8b5dcd5e73144a1099c47708a8db195d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f579070eab0426a98b58d2b04525b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74049dfd556e4358a7baaa5621b419d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89392e4ac28d4a3f90d9a1227e505de1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f29c89a164710b90a54e18e1b76ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "601789241e4f4f4b989dafadfc7dde19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190e66177b384643ac35ff794e3fb0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4701ef625a4e49578277c8e7eadefdcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcb76771651b4b8684ad335e8bfeff03",
              "IPY_MODEL_e62bbee0388e4699a30c2b30d802d979",
              "IPY_MODEL_efea2c0998f6430b98645324e550b691"
            ],
            "layout": "IPY_MODEL_09bfab36b5884908a83f723ee62e751d"
          }
        },
        "dcb76771651b4b8684ad335e8bfeff03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f04da22f1ab04cc498da975c45a0266e",
            "placeholder": "​",
            "style": "IPY_MODEL_c797a4b5421c4a03b616c8e32bf0051f",
            "value": "Downloading (…)l-00002-of-00010.bin: 100%"
          }
        },
        "e62bbee0388e4699a30c2b30d802d979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08fb543e3c3a40a08e07461a11a57605",
            "max": 2877403863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fc5a27cef8844019c9b2f5d7e2690a0",
            "value": 2877403863
          }
        },
        "efea2c0998f6430b98645324e550b691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b0660b61df14c18a531d80a701e757a",
            "placeholder": "​",
            "style": "IPY_MODEL_2465971ce5f4453db3156ad9314ddf5d",
            "value": " 2.88G/2.88G [00:15&lt;00:00, 249MB/s]"
          }
        },
        "09bfab36b5884908a83f723ee62e751d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04da22f1ab04cc498da975c45a0266e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c797a4b5421c4a03b616c8e32bf0051f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08fb543e3c3a40a08e07461a11a57605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc5a27cef8844019c9b2f5d7e2690a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b0660b61df14c18a531d80a701e757a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2465971ce5f4453db3156ad9314ddf5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52844abbdd5248aebc8bb60df8be8dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f505a3a0dd34920ac3cffcd70fda47d",
              "IPY_MODEL_21fbb393a23f49ceb4466f6c894a501e",
              "IPY_MODEL_9dc77e6771d644968d4e5a9288f9b299"
            ],
            "layout": "IPY_MODEL_177b2a6bc7ac4b5d92bbb39aa62c9d8e"
          }
        },
        "0f505a3a0dd34920ac3cffcd70fda47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb215d566c664202925e579ac9e83505",
            "placeholder": "​",
            "style": "IPY_MODEL_7cb6f83a234d434ebfef1beb665f4c35",
            "value": "Downloading (…)l-00003-of-00010.bin: 100%"
          }
        },
        "21fbb393a23f49ceb4466f6c894a501e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79857fd6011543ffb71e6bafc671685e",
            "max": 2990682921,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d709dc86ee6b43cbb8b55a751f487ecb",
            "value": 2990682921
          }
        },
        "9dc77e6771d644968d4e5a9288f9b299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922a93c8d09b4c4b90059b42d603a15d",
            "placeholder": "​",
            "style": "IPY_MODEL_2733b4eb89cc49e487bf7f5c2fe897f1",
            "value": " 2.99G/2.99G [00:12&lt;00:00, 198MB/s]"
          }
        },
        "177b2a6bc7ac4b5d92bbb39aa62c9d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb215d566c664202925e579ac9e83505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb6f83a234d434ebfef1beb665f4c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79857fd6011543ffb71e6bafc671685e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d709dc86ee6b43cbb8b55a751f487ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "922a93c8d09b4c4b90059b42d603a15d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2733b4eb89cc49e487bf7f5c2fe897f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a6ed97cfa194591a740ef5220fbd25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfb5bf2bca4a40cdba3817c4f120d158",
              "IPY_MODEL_09496b3c7c5a482586b2ee2b929ab47c",
              "IPY_MODEL_937e27badb7a43e0ac9505e47ac21787"
            ],
            "layout": "IPY_MODEL_c39b3a80b85c4a09b9513fce102790de"
          }
        },
        "cfb5bf2bca4a40cdba3817c4f120d158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11a2069c74904b63bafd87005342162e",
            "placeholder": "​",
            "style": "IPY_MODEL_b8bf9d45ed1e40b98a36b2fa0f6a7e1f",
            "value": "Downloading (…)l-00004-of-00010.bin: 100%"
          }
        },
        "09496b3c7c5a482586b2ee2b929ab47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_655a2b41c9084b69acaa243d64201b37",
            "max": 2856431667,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e967816905d478581c77847b51a8c22",
            "value": 2856431667
          }
        },
        "937e27badb7a43e0ac9505e47ac21787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46d5ac0ee6c49f9982ecda46504b1b6",
            "placeholder": "​",
            "style": "IPY_MODEL_e6444926b2ad4a04a3ab1cad0a6c6f7c",
            "value": " 2.86G/2.86G [03:59&lt;00:00, 19.1MB/s]"
          }
        },
        "c39b3a80b85c4a09b9513fce102790de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a2069c74904b63bafd87005342162e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8bf9d45ed1e40b98a36b2fa0f6a7e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "655a2b41c9084b69acaa243d64201b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e967816905d478581c77847b51a8c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d46d5ac0ee6c49f9982ecda46504b1b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6444926b2ad4a04a3ab1cad0a6c6f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575f60da6b4c46258bd61cf9bbda2196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ebf092f7abb49e88883ca78dde3ce4c",
              "IPY_MODEL_dd7a36d8333548f596642d1d62eca92c",
              "IPY_MODEL_4525c0d67d0e48e485325962b9a3fffb"
            ],
            "layout": "IPY_MODEL_3678eca8a0ee4734bfa31778bca93098"
          }
        },
        "3ebf092f7abb49e88883ca78dde3ce4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0feb27361d914c75950d9eb8dab43c82",
            "placeholder": "​",
            "style": "IPY_MODEL_0deaabc1a5ec42d1a3b6cb068a145cf3",
            "value": "Downloading (…)l-00005-of-00010.bin: 100%"
          }
        },
        "dd7a36d8333548f596642d1d62eca92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d146616507a447580fbe5350dcee2da",
            "max": 2877437307,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5659ff3b31946328d65f089f4aabb4f",
            "value": 2877437307
          }
        },
        "4525c0d67d0e48e485325962b9a3fffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3d98b27d5cd4e80a1be934ebb74d54a",
            "placeholder": "​",
            "style": "IPY_MODEL_037d7d6cded44ad6bdd79e8be753da30",
            "value": " 2.88G/2.88G [00:15&lt;00:00, 239MB/s]"
          }
        },
        "3678eca8a0ee4734bfa31778bca93098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0feb27361d914c75950d9eb8dab43c82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0deaabc1a5ec42d1a3b6cb068a145cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d146616507a447580fbe5350dcee2da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5659ff3b31946328d65f089f4aabb4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3d98b27d5cd4e80a1be934ebb74d54a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "037d7d6cded44ad6bdd79e8be753da30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b292f3c1cd4766b0172e65f16b35d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be7dfd3115034b858111b7093174fe52",
              "IPY_MODEL_67c35692011249f0b2974e87027ea49d",
              "IPY_MODEL_8832aa29b4104cd49b31a86eb8036408"
            ],
            "layout": "IPY_MODEL_0da06dc077c046db859724af55b7b560"
          }
        },
        "be7dfd3115034b858111b7093174fe52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9eb259f9dff466dba6ba7aad1aeec30",
            "placeholder": "​",
            "style": "IPY_MODEL_de87b9dada0340fd9aab6a9a69215c19",
            "value": "Downloading (…)l-00006-of-00010.bin: 100%"
          }
        },
        "67c35692011249f0b2974e87027ea49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399a1a4bddce4359b4a49b69c943da2d",
            "max": 2969710725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_705ce7fe745b467aac39e11cd805ef00",
            "value": 2969710725
          }
        },
        "8832aa29b4104cd49b31a86eb8036408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db7415630f664fc7a93e16af575f97a0",
            "placeholder": "​",
            "style": "IPY_MODEL_02895c9f2d0e4be0928d624b5314e1a9",
            "value": " 2.97G/2.97G [00:16&lt;00:00, 243MB/s]"
          }
        },
        "0da06dc077c046db859724af55b7b560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9eb259f9dff466dba6ba7aad1aeec30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de87b9dada0340fd9aab6a9a69215c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "399a1a4bddce4359b4a49b69c943da2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "705ce7fe745b467aac39e11cd805ef00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db7415630f664fc7a93e16af575f97a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02895c9f2d0e4be0928d624b5314e1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0083d7c30a94c13863e1b02a14fcf14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58b6d9bd13bd462c96211eefb2c060aa",
              "IPY_MODEL_29fa906cc9324060a99418f15d773fff",
              "IPY_MODEL_5b69dd5ad1884c30b3b40586b376b1dd"
            ],
            "layout": "IPY_MODEL_71300263b6fe4307b0f744bec7fd6a3b"
          }
        },
        "58b6d9bd13bd462c96211eefb2c060aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_453adbb6182240f588358cf1ea3fb417",
            "placeholder": "​",
            "style": "IPY_MODEL_06b721de73b3467fb285aee7696d4d6e",
            "value": "Downloading (…)l-00007-of-00010.bin: 100%"
          }
        },
        "29fa906cc9324060a99418f15d773fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c85815434c874134b9262159771a2c18",
            "max": 2877403863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5048654f32ef4f0a81af1ac25b15b380",
            "value": 2877403863
          }
        },
        "5b69dd5ad1884c30b3b40586b376b1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d9b44d937d4064a7f610bf0465e89b",
            "placeholder": "​",
            "style": "IPY_MODEL_7501e83aeb3c4cc6bb891a88a757ddc6",
            "value": " 2.88G/2.88G [00:15&lt;00:00, 176MB/s]"
          }
        },
        "71300263b6fe4307b0f744bec7fd6a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "453adbb6182240f588358cf1ea3fb417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06b721de73b3467fb285aee7696d4d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c85815434c874134b9262159771a2c18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5048654f32ef4f0a81af1ac25b15b380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9d9b44d937d4064a7f610bf0465e89b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7501e83aeb3c4cc6bb891a88a757ddc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2a3887452984984ae7773cf275405dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21310fdd25e84c09aac4590cb43a0434",
              "IPY_MODEL_47df6a814f0d4f61988a438698bdbbcc",
              "IPY_MODEL_46c436c148ed42738f978a6721259e5f"
            ],
            "layout": "IPY_MODEL_0fac26f4b55b4845a6a28ba5827dbadc"
          }
        },
        "21310fdd25e84c09aac4590cb43a0434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e56eb71b8e4dd397f37014ce265f09",
            "placeholder": "​",
            "style": "IPY_MODEL_de72dfc7a68448079bf9cb96e9a0852d",
            "value": "Downloading (…)l-00008-of-00010.bin: 100%"
          }
        },
        "47df6a814f0d4f61988a438698bdbbcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8426efa0f9f449cfbd0b354d2fb7ec62",
            "max": 2990682921,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15ec9f3cbbb3443a89a7acf1b52459fc",
            "value": 2990682921
          }
        },
        "46c436c148ed42738f978a6721259e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04d1cbb39a1c4abaae5b4b2ab1e5e18e",
            "placeholder": "​",
            "style": "IPY_MODEL_91447c43539f4407ad2838b9a12a8150",
            "value": " 2.99G/2.99G [00:11&lt;00:00, 273MB/s]"
          }
        },
        "0fac26f4b55b4845a6a28ba5827dbadc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e56eb71b8e4dd397f37014ce265f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de72dfc7a68448079bf9cb96e9a0852d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8426efa0f9f449cfbd0b354d2fb7ec62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15ec9f3cbbb3443a89a7acf1b52459fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04d1cbb39a1c4abaae5b4b2ab1e5e18e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91447c43539f4407ad2838b9a12a8150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65e9055aa9b2460c864076bfdc865868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_078f8c29563a4ad08d676237a0cf6298",
              "IPY_MODEL_071c7761c13d4f71b47c4efc4b670813",
              "IPY_MODEL_632b42a9dd2549169544a649b6962387"
            ],
            "layout": "IPY_MODEL_05e2a3f876cf4432a0917f867a09eed3"
          }
        },
        "078f8c29563a4ad08d676237a0cf6298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9acb4c4e4124b9bb76c1b5519d03461",
            "placeholder": "​",
            "style": "IPY_MODEL_b1cd73f50d984c02949e8f6bd71a6713",
            "value": "Downloading (…)l-00009-of-00010.bin: 100%"
          }
        },
        "071c7761c13d4f71b47c4efc4b670813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0cdd47363c4d758afc0ca48bd61524",
            "max": 2856431667,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_827378ba4f8e47ac982a5c211f632a82",
            "value": 2856431667
          }
        },
        "632b42a9dd2549169544a649b6962387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1676f9e03db74556bc64479c42893b2c",
            "placeholder": "​",
            "style": "IPY_MODEL_e31a5b0f25e245f69f1371c704d7c6e0",
            "value": " 2.86G/2.86G [00:11&lt;00:00, 244MB/s]"
          }
        },
        "05e2a3f876cf4432a0917f867a09eed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9acb4c4e4124b9bb76c1b5519d03461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1cd73f50d984c02949e8f6bd71a6713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c0cdd47363c4d758afc0ca48bd61524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827378ba4f8e47ac982a5c211f632a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1676f9e03db74556bc64479c42893b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e31a5b0f25e245f69f1371c704d7c6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b580f04efa949728ebb0960faae7d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bca494820d64308b1715417f19b1b9d",
              "IPY_MODEL_e91c96a264f84b32a1fa21356e1fb657",
              "IPY_MODEL_dd16798f7fcd4a478a824d0cfb26e47c"
            ],
            "layout": "IPY_MODEL_3883d62462ad48eea31c9fdf11cee093"
          }
        },
        "6bca494820d64308b1715417f19b1b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769085ae00b640a0a4724729d140b3e6",
            "placeholder": "​",
            "style": "IPY_MODEL_620ec9dcf65144c5acff053f3e02c543",
            "value": "Downloading (…)l-00010-of-00010.bin: 100%"
          }
        },
        "e91c96a264f84b32a1fa21356e1fb657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7dbc5eea5f84f3182bc0564002d808a",
            "max": 704694382,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_172761beaedf4d898ac9efc1f2200e2c",
            "value": 704694382
          }
        },
        "dd16798f7fcd4a478a824d0cfb26e47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4d8d6276714bc78583ab9001f0b4b2",
            "placeholder": "​",
            "style": "IPY_MODEL_0c134b56af8942558afa200f8bd10587",
            "value": " 705M/705M [00:03&lt;00:00, 256MB/s]"
          }
        },
        "3883d62462ad48eea31c9fdf11cee093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "769085ae00b640a0a4724729d140b3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620ec9dcf65144c5acff053f3e02c543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7dbc5eea5f84f3182bc0564002d808a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "172761beaedf4d898ac9efc1f2200e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d4d8d6276714bc78583ab9001f0b4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c134b56af8942558afa200f8bd10587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6a32f24aee940199a729906503a001a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b5b356d6ef742deb3aa9efa204de3e4",
              "IPY_MODEL_ee3e2c657a2145fa870745caeac7e535",
              "IPY_MODEL_2d554f3a999c45959638790ec61b48e4"
            ],
            "layout": "IPY_MODEL_4a863bff7b474e3c8057a494a1f9094a"
          }
        },
        "5b5b356d6ef742deb3aa9efa204de3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba78d5eb73f4c6fbe63f133b5d9d076",
            "placeholder": "​",
            "style": "IPY_MODEL_244613ffc5dd4cc2b2cd1977f17e7777",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ee3e2c657a2145fa870745caeac7e535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_465c5f52b5f24846b7f402ed5f80693c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f32cbb59daf740eebbe8077d4045d425",
            "value": 10
          }
        },
        "2d554f3a999c45959638790ec61b48e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03183ae108c046638b7f0ce7189eae77",
            "placeholder": "​",
            "style": "IPY_MODEL_b8e1a9e9f3904da58b0dc20854384aaf",
            "value": " 10/10 [02:23&lt;00:00, 11.85s/it]"
          }
        },
        "4a863bff7b474e3c8057a494a1f9094a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba78d5eb73f4c6fbe63f133b5d9d076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244613ffc5dd4cc2b2cd1977f17e7777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "465c5f52b5f24846b7f402ed5f80693c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f32cbb59daf740eebbe8077d4045d425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03183ae108c046638b7f0ce7189eae77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e1a9e9f3904da58b0dc20854384aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d0cd744c0b645feb4e4a1c5976c5fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_407f477ce15c421eb9a30bc2ef2cc026",
              "IPY_MODEL_613635c363594cfa8cf47ad03fc2edbb",
              "IPY_MODEL_9a6922f4b06d4d5aa6e93ec70c476478"
            ],
            "layout": "IPY_MODEL_90c9854e7e8244419591e8c5f3cdf499"
          }
        },
        "407f477ce15c421eb9a30bc2ef2cc026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4eac48bfed54ad4bbec93a03a61928f",
            "placeholder": "​",
            "style": "IPY_MODEL_4451182adaa24c67b4783038e7be414f",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "613635c363594cfa8cf47ad03fc2edbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb77ad3db2974df990bf32ae5e6587a7",
            "max": 174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98ebb1f377ae42179437216ae07581b5",
            "value": 174
          }
        },
        "9a6922f4b06d4d5aa6e93ec70c476478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675e964cf4fc4a7997d43642641bff4a",
            "placeholder": "​",
            "style": "IPY_MODEL_e6517290a69d494ab11fb39b3bed8a97",
            "value": " 174/174 [00:00&lt;00:00, 10.5kB/s]"
          }
        },
        "90c9854e7e8244419591e8c5f3cdf499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4eac48bfed54ad4bbec93a03a61928f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4451182adaa24c67b4783038e7be414f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb77ad3db2974df990bf32ae5e6587a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ebb1f377ae42179437216ae07581b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "675e964cf4fc4a7997d43642641bff4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6517290a69d494ab11fb39b3bed8a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/langchain_colab_experiments/blob/main/chat_with_multiple_documents_llama2_openai_chroma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "rHfhuOcFIM6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsA6xR7oGmoZ",
        "outputId": "ce597140-e7d5-45f2-fe8e-5ed8c9688bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install pypdf\n",
        "\n",
        "!pip -q install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHQbIhq3OMB-",
        "outputId": "caa2a031-6925-47f8-c798-ffed11ea85bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.3-py3-none-any.whl (399 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/399.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/399.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/399.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.0/399.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Collecting chroma-hnswlib==0.7.1 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=2271627 sha256=1cb1c100b3e3ec4e479819571b569dae1f6d0ad9aaa8638fa54b55256e7c5058\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=aea7c9489f3623b0925b97fea98a3cb587f3cb1c425ea9b281765d1007ac84cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.3 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 pypika-0.48.9 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS3BVnVbyAYr",
        "outputId": "14c0bd3a-b9fe-46d8-8d82-5205c81ea0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/73.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "wdMi5csDI57_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "fnCwE83cIpaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "NsPrXLUBJGs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import Docx2txtLoader\n"
      ],
      "metadata": {
        "id": "Rf0I9tVUKEk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "UnQ1wQ3sLDCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "CwL28Bl6MtBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "nW6LntbfNZYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "G2y6OYAkObHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "X9DiVACTPfWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "8d4O7Jz8PfbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "7dW0NL5gQ2Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "5d5fB9MXSGV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "Ht8TUPXsRr0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "PvePppDWSxJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "4m8LiUl4xIO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "0XXyzEW1xIVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "eh3j9VS5JLQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "8eRHhti8B-Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Load the Documents and Extract Text From Them**"
      ],
      "metadata": {
        "id": "4micSehLLxUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir docs"
      ],
      "metadata": {
        "id": "arhUc1aPJNSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78978d84-5004-4fa9-ef67-fdb0f98f50e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘docs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document=[]\n",
        "for file in os.listdir(\"docs\"):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    pdf_path=\"./docs/\"+file\n",
        "    print(pdf_path)\n",
        "    loader=PyPDFLoader(pdf_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "    doc_path=\"./docs/\"+file\n",
        "    loader=Docx2txtLoader(doc_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.txt'):\n",
        "    text_path=\"./docs/\"+file\n",
        "    loader=TextLoader(text_path)\n",
        "    document.extend(loader.load())"
      ],
      "metadata": {
        "id": "itRfVkSfJhaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0f5ae7-19e6-4fba-d059-fba0743b76e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./docs/KG-LLM.pdf\n",
            "./docs/distilling-LLM.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl9p848iKzK8",
        "outputId": "99473248-5d21-4bc6-f439-dc0f669f9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='SKILL: Structured Knowledge Infusion for Large Language Models\\nFedor Moiseev∗1,2Zhe Dong†2Enrique Alfonseca2Martin Jaggi1\\n1EPFL, Switzerland2Google, Switzerland\\n{femoiseev, zhedong, ealfonseca}@google.com, martin.jaggi@epfl.ch\\nAbstract\\nLarge language models (LLMs) have demon-\\nstrated human-level performance on a vast\\nspectrum of natural language tasks. However,\\nit is largely unexplored whether they can bet-\\nter internalize knowledge from a structured\\ndata, such as a knowledge graph, or from text.\\nIn this work, we propose a method to infuse\\nstructured knowledge into LLMs, by directly\\ntraining T5 models on factual triples of knowl-\\nedge graphs (KGs). We show that models pre-\\ntrained on Wikidata KG with our method out-\\nperform the T5 baselines on FreebaseQA and\\nWikiHop, as well as the Wikidata-answerable\\nsubset of TriviaQA and NaturalQuestions. The\\nmodels pre-trained on factual triples compare\\ncompetitively with the ones on natural lan-\\nguage sentences that contain the same knowl-\\nedge. Trained on a smaller size KG, Wiki-\\nMovies, we saw 3×improvement of exact\\nmatch score on MetaQA task compared to T5\\nbaseline. The proposed method has an advan-\\ntage that no alignment between the knowledge\\ngraph and text corpus is required in curating\\ntraining data. This makes our method particu-\\nlarly useful when working with industry-scale\\nknowledge graphs.\\n1 Introduction\\nLarge pre-trained language models, such as BERT\\n(Devlin et al., 2019), GPT-3 (Brown et al., 2020),\\nT5 (Raffel et al., 2019), REALM (Guu et al., 2020)\\nand ERNIE (Sun et al., 2021) have become the\\nstate-of-the-art technology for many tasks. They\\nare commonly pre-trained using unstructured text\\ncorpora, on tasks such as next word prediction,\\nnext sentence prediction (NSP) or masked lan-\\nguage modelling (MLM). Especially for T5, self-\\nsupervised learning on unlabelled text corpus with\\nMLM has been a common pre-training recipe\\n(Roberts et al., 2020). This is normally followed\\n∗Work done during internship at Google.\\n†Correspondence Author.by a ﬁne-tuning step on the task of interest (Ruder\\net al., 2019), although large language models have\\nalso proved useful without this task-speciﬁc ﬁne-\\ntuning (Brown et al., 2020).\\nBeyond the capacity of contextual understand-\\ning, human-level language understanding pivots on\\nthe knowledge about the world. The world knowl-\\nedge is often expressed as factual triples (c.f. Ji\\net al., 2020), in the form of ( subject entity ,relation ,\\nobject entity ). A knowledge graph (KG) deﬁned by\\na set of factual triples consists of the subjects and\\nobjects as vertices/nodes, and the relations form-\\ning the edges connecting them. Most of the large\\nscale KGs (e.g. Wikidata, Vrande ˇci´c and Krötzsch,\\n2014) are stored in triple format.\\nLLMs demonstrate some capacity of learning\\nworld knowledge from the natural text corpus\\n(Roberts et al., 2020), but it is unclear to what\\ndegree they are also able to learn and memorize\\nnew knowledge directly from structured KG triples,\\nor from text describing them explicitly.\\nIn order to infuse knowledge into a LLM, one\\noption is to generate a textual version of the knowl-\\nedge base, and apply the standard training objec-\\ntives, e.g. MLM. This is unfortunately highly non-\\ntrivial. One can either align sentences with KG\\ntriples, as done in ERNIE (Sun et al., 2021), or\\ngenerate sentences from triples, as done in KELM\\n(Agarwal et al., 2021). These approaches are un-\\nfortunately hard to port to knowledge graphs with\\ndifferent schemas. These processes are also lossy\\nin that not every triple can be aligned or produce\\na valid sentence, and there is not a good under-\\nstanding whether this can introduce unnecessary\\nselection biases on top of biases existing in the\\noriginal KG.\\nIn this work, we propose a method of Knowl-\\nedge Infusion for Large Language Models\\n(SKILL) , where LLMs directly learns from knowl-\\nedge triples. Experiment results shows the check-\\npoints trained with proposed method on WikidataarXiv:2205.08184v1  [cs.CL]  17 May 2022', metadata={'source': './docs/KG-LLM.pdf', 'page': 0}),\n",
              " Document(page_content='KG outperform the T5 baselines on four standard\\nclosed-book question-answering (QA) tasks. With\\na smaller KG, WikiMovies, the proposed method\\ngain3×exact match score performance improve-\\nment on MetaQA task. The models learning di-\\nrectly from knowledge triples performs competi-\\ntively with the ones with the aligned natural sen-\\ntences that contain the same amount of knowledge.\\nBeing able to learn directly from knowledge triples\\nenables easy addition of structured knowledge into\\nlanguage modeling pre-training.\\n2 Related work\\nPrevious works that use knowledge graphs to en-\\nhance the quality of knowledge-intensive down-\\nstream tasks can be divided into two groups: using\\nknowledge graphs at the inference time, and in-\\nfusing knowledge into the model weights at the\\npre-training time. The proposed method falls in the\\nlatter group.\\nExplicit usage of knowledge graphs. A\\nretrieval-augmented model is commonly used,\\nin order to retrieve and apply the knowledge\\nfrom external memories or sources. FILM (Verga\\net al., 2021) and EaE (Févry et al., 2020) extend\\nTransformer (Vaswani et al., 2017) models with\\nexternal entity (both FILM and EaE) and fact\\n(FILM) memories. REALM (Guu et al., 2020)\\nis pre-trained to perform reasoning over a large\\ntextual knowledge corpus on-the-ﬂy during infer-\\nence. UniK-QA (Oguz et al., 2020) combines the\\nstructured and unstructured information to improve\\nthe open-domain QA tasks with a retriever-reader\\nframework. The main difference between the\\nproposed method, SKILL, and retrieval-augmented\\nmodels is that SKILL doesn’t introduce retrieval\\nsystem or external memories to the model, but\\nit directly embeds knowledge into the model\\nparameters, which introduces no extra cost at\\ninference time.\\nKnowledge infusion. A common way of param-\\neterized knowledge infusion is to map or convert\\nstructured knowledges into natural language text.\\nERNIE 3.0 (Sun et al., 2021) trains a knowledge-\\nenhanced model on a corpus combining triples and\\ntheir aligned sentences, by randomly masking re-\\nlation in a triple or words in a sentence. On the\\ncontrary, SKILL trains only on triples.\\nKnowBert (Peters et al., 2019) incorporates\\nknowledge from Wikipedia and WordNet (Miller,1995) into a BERT model through entity\\nembeddings with knowledge-attention and re-\\ncontextualization mechanism. BERT-MK (He et al.,\\n2020) is a BERT-based model that integrates graph\\ncontextual knowledge of a medical KG, which\\ndemonstrates the utility of graph-level knowledge.\\nThese approaches requires entity linking and sen-\\ntences contextualizing the knowledge graph infor-\\nmation.\\nKG-FiD (Yu et al., 2021) extends the Fusion-in-\\nDecoder model (Izacard and Grave, 2021), with a\\nmodule that ﬁlters and re-ranks passages based on\\nstructural connections in knowledge graph between\\nentities described in those passages. In contrast to\\nthe SKILL method that we propose, it requires\\nthe existence of natural text passages describing\\neach knowledge graph entity, so Wikipedia corpus\\nwas used since it naturally provides articles that\\ndescribe entities.\\nHeinzerling and Inui (2021) explored the ability\\nof language models to memorize and understand\\ninformation from knowledge graphs, but used nat-\\nural language representation of triples based on\\npredeﬁned templates instead of structured represen-\\ntation. Usage of predeﬁned templates signiﬁcantly\\nlimits scalability and therefore only relatively small\\nknowledge graphs were used, such as Google-RE1.\\nIn contrast to the new method presented in this\\npaper, all of these approaches require an explicit\\nmapping between the knowledge graph entities\\nor facts and corresponding natural language sen-\\ntences, which can limit applications to industry-\\nscale knowledge graphs that don’t have such a map-\\nping.\\nDifferent goals of using knowledge graphs.\\nBesides that, some papers embed knowledge into\\nmodel weights but pursue different goals rather\\nthan improving performance on downstream tasks.\\nCOMET (Bosselut et al., 2019) is most similar to\\nour work and trains a commonsense-aware Trans-\\nformer Language Model by learning to generate\\nloosely structured commonsense descriptions in the\\nnatural language given the structured knowledge.\\nSimilar to us, it also uses KG triples in surface\\nform as a source for training data, but in contrast\\nto our research, the ﬁnal goal of COMET is to gen-\\nerate new knowledge instead of utilizing existing\\nones. Another important difference is the scale:\\nCOMET uses Atomic (Sap et al., 2019) and Con-\\n1https://ai.googleblog.com/2013/04/50000-lessons-on-\\nhow-to-read-relation.html', metadata={'source': './docs/KG-LLM.pdf', 'page': 1}),\n",
              " Document(page_content='ceptNet (Speer et al., 2017) Knowledge Graphs\\nthat are much smaller than Wikidata (Vrande ˇci´c\\nand Krötzsch, 2014).\\nKELM (Agarwal et al., 2021) ﬁne-tunes a T5\\nmodel to convert KGs to synthetic natural language\\nsentences to augment existing pre-training corpora.\\nWe build our research on top of it and use the\\nKELM dataset to compare structured and natural\\nlanguage representations of knowledge.\\n3 Method\\nThere are two components of knowledge infusion\\nfor LLMs (SKILL): the corpus and the training\\nmethod. We introduce the method based on Wiki-\\ndata KG, but it can be applied to any other KGs.\\nTraining corpus. We use two corpora with dif-\\nferent knowledge representations: Wikidata KG\\n(Vrande ˇci´c and Krötzsch, 2014) in triple format,\\nand KELM corpus2(Agarwal et al., 2021) as\\nsynthetic natural language sentences converted\\nfrom Wikidata KG. The KELM corpus contains\\n15,628,486synthetic sentences. To ensure two\\ncorpora share the same knowledge, we take the\\nsnapshot of the Wikidata KG used to created the\\nKELM corpus, which contains 35,697,715triples.\\nTo prevent the degradation of model perfor-\\nmance on natural language understanding, we mix\\nthe Wikidata corpus or KELM corpus with natural\\ntext from C4 (Raffel et al., 2019), 50 : 50 , for the\\nknowledge infusion training data.\\nTraining method. T5 (Raffel et al., 2019) was\\ntrained through masked-language modelling with\\nrandom span corruption on the C4 corpus. Roberts\\net al. (2020) found that masking salient terms (Guu\\net al., 2020) in pre-training T5 models, instead of\\nmasking random token spans, could signiﬁcantly\\nimprove the performance on downstream tasks, e.g.\\nclosed-book QA.\\nWe apply salient span masking for unsupervised\\nlearning in our knowledge-infusing training. To\\nmask the same amount of information is for both\\ncorpora, the following method is applied. For a\\nknowledge triple, we mask either the subject or\\nobject entity. For a KELM sentence, we identify\\nthe aligned triple, with details in Appendix A, and\\nmask the full spans corresponding to the subject\\nor object in the triple. The relation tokens are\\nnever masked, as there is no robust way to map\\n2Data is available at https://github.com/google-research-\\ndatasets/KELM-corpusthe abstract relation in knowledge triples to natural\\nlanguage tokens in KELM sentences. Examples of\\nthe inputs for both corpora are in Table 1.\\n4 Experiments\\nWe assess SKILL by training and evaluating the\\nknowledge infused models on closed-book QA\\ntasks, where questions are provided without sup-\\nporting context and external knowledge.\\n4.1 Experiment Setup\\nSKILL pre-training. We apply SKILL on three\\nT5.1.1 pre-trained checkpoints3, base, large, and\\nXXL, with sizes of ∼250M,∼800M and∼11B\\nparameters, respectively. For T5.1.1-base and -\\nlarge, SKILL training is performed for 500K steps\\nwith batch size 1024 , which translates to ∼7.17\\nepochs on Wikidata KG and ∼16.38epochs in\\nKELM sentences. For T5.1.1-XXL, the model is\\ntrained for 100K steps to ﬁnish training in a feasible\\ntime.\\nAs baseline we use pre-trained T5 checkpoints\\nof the same size. To make sure that improvements\\ncome from knowledge infusion instead of from\\nlonger C4 pre-training, we use a second baseline by\\nfurther training the T5 checkpoints on C4 for half\\nof the aforementioned steps, to match the amount\\nof C4 pre-training used in SKILL.\\nAll the model variations are optimized by\\nAdaFactor (Shazeer and Stern, 2018) with 10−3\\nlearning rate and 0.1dropout rate, the same set-\\ntings that were used for T5.\\nFine-tuning on closed-book QA tasks. We eval-\\nuate the checkpoints by ﬁne-tuning on the fol-\\nlowing QA benchmarks: FreebaseQA (Jiang\\net al., 2019), WikiHop (Welbl et al., 2018), Triv-\\niaQA (Joshi et al., 2017) and NaturalQuestions\\n(Kwiatkowski et al., 2019), with the aforemen-\\ntioned hyper-parameters for optimization and 128\\nbatch size. For the benchmarks without a testsplit,\\nwe use the devsplit for test, and the last 10% of\\ntrain asdevsplit.\\nThe Exact Match (EM) scores on the test sets\\nare calculated after being ﬁne-tuned for 50K steps\\nfor T5.1.1-base and -large models, and 10K steps\\nfor -XXL models. All models converged with no\\nnoticeable over-ﬁtting according to the EM scores\\non validation sets.\\n3https://goo.gle/t5-checkpoints', metadata={'source': './docs/KG-LLM.pdf', 'page': 2}),\n",
              " Document(page_content='Wikidata triple KELM sentence Wikidata input KELM input Target\\n(\"Pulp Fiction\",\\n\"award received\",\\n\"Palme d’Or\")Quentin Tarantino\\nwon the Palme d’Or in 1994\\nfor Pulp Fiction.Pulp Fiction,\\naward received,\\n[MASK]Quentin Tarantino\\nwon the [MASK] in 1994\\nfor Pulp Fiction.Palme d’Or\\nTable 1: Example inputs for SKILL pre-training with Wikidata and KELM corpora.\\nModel FreebaseQA WikiHop TQA-matched TQA NQ-matched NQ\\ndev test dev test dev test dev test dev test dev test\\nbase 25.24 27 .55 19 .09 18 .38 31 .24 33 .55 22 .64 22 .93 36 .64 32 .68 25 .04 25 .48\\nbase + C4 26.19 28 .33 19 .57 19 .36 32 .9 34 .4 24 .54 25 .39 36 .98 32 .03 25.88 25.84\\nbase + WikiKG 26.92 28 .38 20.28 20.22 34 .21 35.08 24 .73 25.77 37 .41 33 .33 25.51 25 .76\\nbase + KELM 26.64 28 .15 20.62 19.81 33 .64 35.54 25 .22 25.75 36 .98 32 .9 25 .31 26.2\\nlarge 30.22 32 .88 20 .92 21 .12 36 .7 38 .09 29 .24 30 .03 39 .22 35 .06 27 .12 27 .15\\nlarge + C4 32.55 34 .01 22 .5 21 .51 38 .78 40 .6 30 .32 30 .83 39 .74 35 .5 27 .46 28 .17\\nlarge + WikiKG 33.22 35 .29 23 .5 23 .4 39.19 41.02 29.74 30 .47 41.12 35 .93 27.38 27 .89\\nlarge + KELM 32.65 34 .16 23 .34 22 .91 39.45 40.76 30.51 30 .65 40.95 35 .527.67 28 .56\\nXXL 43.67 45 .02 24 .76 24 .8 51 .73 53 .1 42 .44 42 .21 46 .47 43 .72 31 32 .27\\nXXL + C4 42.01 44 .14 23 .34 22 .23 50 .59 52 .19 40 .66 40 .99 45 .43 40 .26 30 .35 31 .08\\nXXL + WikiKG 45.22 47.25 27 .57 27 .65 54 .17 54.18 42 .55 43.54 49 .14 44 .37 31.11 32.74\\nXXL + KELM 45.42 45.9 26 .11 26 .26 53 .65 54.21 42 .68 42.95 48 .53 44 .16 31.79 32.6\\nTable 2: Exact match scores achieved by ﬁne-tuning the checkpoints on closed-book QA tasks. base ,large ,\\nXXL represent the corresponding T5.1.1-* checkpoints. *-C4 are the checkpoints additionally trained on C4\\ncorpus as discussed in Section 3. *-WikiKG and*-KELM are the checkpoints trained on Wikidata KG triple\\ncorpus and KELM sentence corpus. The best performed checkpoints are in bold. Details about datasets are in\\nAppendix B.\\nWikidata-answerable QA. We found that the\\nmajority of the questions in FreebaseQA and Wiki-\\nHop can be answered directly from triples in Wiki-\\ndata. This is because FreebaseQA was created by\\nmatching question-answer pairs with triples in Free-\\nbase (Bollacker et al., 2008), most of which was\\nimported into Wikidata (Vrande ˇci´c and Krötzsch,\\n2014). For WikiHop, the questions were generated\\nfrom Wikidata triples.\\nHowever, TriviaQA and NaturalQuestions were\\ncreated independently of Wikidata, and not every\\nquestion can be answered using this knowledge\\nbase. We found frequent freshness issues, e.g. the\\ngolden answer for question \"Who is the largest\\nsupermarket chain in the UK?\" is \"Aldi\", while\\ntoday it would be \"Tesco\". Some other questions\\ncan not be answered by WikiData, e.g. \"Who, dur-\\ning a radio microphone test in 1984 said, ’I just\\nsigned legislation which outlaws Russia forever.\\nThe bombing begins in ﬁve minutes?’\", with the\\ngolden answer \"Ronald Reagan\".\\nTo mitigate this, we created subsets of TriviaQA\\n(TQA) and NaturalQuestions (NQ) that were some-\\nwhat more likely to have answers in Wikidata. We\\nselected all the items for which there exist a triple in\\nWikidata that has the answer either as subject or ob-\\nject, and the other entity in the triple is mentioned\\nin the question. We match the entities by entityname, case-insensitive. We name the Wikidata-\\naligned version of TQA and NQ as TQA-matched\\nand NQ-matched, respectively. The dataset sizes\\nof all QA tasks are summarized in Appendix B.\\n4.2 Results\\nThe results for closed-book QA tasks are sum-\\nmarized in Table 2. SKILL pre-trained models\\nshow improvements on FreebaseQA, WikiHop, and\\nWikidata-answerable versions of TriviaQA and Nat-\\nuralQuestions, but no signiﬁcant improvement on\\noriginal TriviaQA and NaturalQuestions. As dis-\\ncussed in previous section, we believe this is due\\nto the misalignment between the datasets and Wiki-\\ndata.\\nModels pre-trained on Wikidata KG gives com-\\npetitive results with ones on KELM sentences. It\\nshows that the triple representation is as good as\\nnatural language representation, while being much\\neasier to scale up for larger KG.\\nFor T5.1.1-base and -large, additional pre-\\ntraining on C4 boosts performance in comparison\\nto the original baseline. For T5.1.1-XXL, this addi-\\ntional pre-training leads to a performance regress.\\nIn (Raffel et al., 2019), it is mentioned that training\\non C4 for multiple times may reduce the perfor-\\nmance of a T5 model.', metadata={'source': './docs/KG-LLM.pdf', 'page': 3}),\n",
              " Document(page_content='Figure 1: Performance improvements on closed-\\nbook QA tasks for different model sizes. The im-\\nprovements are measured by the difference of ex-\\nact match score ( ∆EM) between knowledge-infused\\nmodel trained with Wikidata triples and the baseline\\ntrained with C4 corpus.\\nImpact of model size. As shown in Figure 1,\\nSKILL pre-training introduces bigger improve-\\nments when applied on larger models. With more\\nthan35M triples in Wikidata KG, it is harder for\\nsmaller size models, e.g. T5.1.1.-base with 300M\\nparameters, to memorize them efﬁciently. We view\\nthis as an encouraging result, suggesting that as\\nmodel size grows, gains from SKILL pre-training\\nmay increase further.\\nPerformance on a smaller KG. The Wiki-\\nMovies KG (Miller et al., 2016) contains 134,741\\ntriples. T5.1.1-large should have enough parame-\\nters to memorize the KG. We train a T5.1.1-large\\nmodel on the KG for 100K steps,∼380epochs,\\nwith the same hyperparameters as for Wikidata KG.\\nWe evaluate the checkpoints with MetaQA (Zhang\\net al., 2018) benchmark that was constructed over\\nWikiMovies KG. The benchmark contains 3 dif-\\nferent sub-tasks: 1-hop QA (e.g. \"What ﬁlms\\ndoes Paresh Rawal appear in?\"), 2-hop QA (e.g.\\n\"Who are the directors of the ﬁlms written by Laura\\nKerr?\"), 3-hop QA (e.g. \"Who directed the movies\\nwritten by the writer of Millennium Actress?\").\\nThe results in Table 3 demonstrate the effective-\\nness of SKILL pre-training, when it’s possible to\\nmemorize the whole knowledge graph.\\nAs 1-hop questions are supported by single\\ntriples in the WikiMovies KG, a 3×improvement\\non EM score is observed for the sub-task. In or-\\nder to answer 2/3-hop questions it is not enough to\\nmemorize the triples, the model needs to be able\\nto reason with them. This requires a better un-\\nderstanding of the graph structure. Training with\\nsingle triples may not be enough, and the observedDataset Split Baseline + C4 + KG\\n1-hopdev 24.3 23 .12 71 .52\\ntest 24.5 23 .53 71 .47\\n2-hopdev 32.05 32 .23 33 .49\\ntest 32.65 32 .78 33 .57\\n3-hopdev 42.08 39 .22 43 .79\\ntest 42.31 39 .66 43 .41\\nTable 3: Exact match scores achieved by ﬁne-tuning\\ndifferent T5.1.1-large checkpoints on MetaQA task.\\nimprovement is notably smaller. The performance\\ncould be further improved by representing more\\nexplicitly the graph structure in the training data,\\nwhich we leave for future work.\\n5 Conclusion\\nWe proposed a method to directly infuse knowledge\\nfrom knowledge graphs into T5 models through\\npre-training. Empirical results show that T5 can\\nlearn directly from structured data and apply the\\nlearned knowledge to improve closed-book QA\\nresults. We also demonstrated that the models\\npre-trained on factual triples perform competitively\\nwith the ones on natural language sentences that\\ncontain the same knowledge. By enabling knowl-\\nedge infusion directly from triples, this method can\\nbe very easily applied to industry-scale KGs.\\n6 Ethical and Broader Impact\\nIn this work, we are introducing a new method\\nto pre-train a well known natural language under-\\nstanding model, T5, on the full corpora of public\\nknowledge graphs. To the best of our knowledge,\\nthe method will not introduce extra bias to either\\nthe model or the dataset beyond the one potentially\\ninherited from Wikidata (Vrande ˇci´c and Krötzsch,\\n2014) and WikiMovies (Miller et al., 2016) knowl-\\nedge graphs. On the other hand, through knowl-\\nedge fusion pre-training introduced in this work,\\na language model will be able to learn factual\\ninformation to improve the quality of parameter-\\nized knowledge embedded in the model, which is\\ndemonstrated by improvements on various closed-\\nbook question-answering tasks. The proposed\\nmethod and recipe will provide positive impact\\nto the natural language processing community and\\nhelp to improve the factualness in pre-trained large\\nlanguage model checkpoints.\\nLimitations. A factual triple is the basic ingredi-\\nent of a knowledge graph. However, as a seman-\\ntic network, the graph structure of a knowledge', metadata={'source': './docs/KG-LLM.pdf', 'page': 4}),\n",
              " Document(page_content='graph describes how the factual triples are con-\\nnected. This information is not easy to directly\\nrepresent by random set of triples. We leave the ex-\\nploration of how to infuse the information implied\\nby the graph structure for future work. We expect\\nthat this will further improve the results, especially\\nfor multi-hop question-answering tasks.\\nReferences\\nOshin Agarwal, Heming Ge, Siamak Shakeri, and\\nRami Al-Rfou. 2021. Knowledge graph based syn-\\nthetic corpus generation for knowledge-enhanced\\nlanguage model pre-training. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 3554–3565, On-\\nline. Association for Computational Linguistics.\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\\nSturge, and Jamie Taylor. 2008. Freebase: A collab-\\noratively created graph database for structuring hu-\\nman knowledge. In Proceedings of the 2008 ACM\\nSIGMOD International Conference on Management\\nof Data , SIGMOD ’08, page 1247–1250, New York,\\nNY , USA. Association for Computing Machinery.\\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\\n2019. COMET: Commonsense transformers for au-\\ntomatic knowledge graph construction. In Proceed-\\nings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , pages 4762–4779,\\nFlorence, Italy. Association for Computational Lin-\\nguistics.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\\nV oss, Gretchen Krueger, Tom Henighan, Rewon\\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\\nClemens Winter, Chris Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In\\nAdvances in Neural Information Processing Systems ,\\nvolume 33.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\\ntities as experts: Sparse memory access with entitysupervision. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Process-\\ning (EMNLP) , pages 4937–4951, Online. Associa-\\ntion for Computational Linguistics.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Inter-\\nnational Conference on Machine Learning , pages\\n3929–3938. PMLR.\\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\\nNicholas Jing Yuan, and Tong Xu. 2020. BERT-\\nMK: Integrating graph contextualized knowledge\\ninto pre-trained language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2020 , pages 2281–2290, Online. Association for\\nComputational Linguistics.\\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\\nguage models as knowledge bases: On entity\\nrepresentations, storage capacity, and paraphrased\\nqueries. In Proceedings of the 16th Conference of\\nthe European Chapter of the Association for Com-\\nputational Linguistics: Main Volume , pages 1772–\\n1791, Online. Association for Computational Lin-\\nguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 874–880, Online. Association for Com-\\nputational Linguistics.\\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti-\\nnen, and Philip S. Yu. 2020. A survey on knowledge\\ngraphs: Representation, acquisition and applications.\\nCoRR , abs/2002.00388.\\nKelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-\\nbaseQA: A new factoid QA data set matching trivia-\\nstyle question-answer pairs with Freebase. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long and Short Papers) , pages 318–323,\\nMinneapolis, Minnesota. Association for Computa-\\ntional Linguistics.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,', metadata={'source': './docs/KG-LLM.pdf', 'page': 5}),\n",
              " Document(page_content='Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\\nHossein Karimi, Antoine Bordes, and Jason Weston.\\n2016. Key-value memory networks for directly read-\\ning documents. In Proceedings of the 2016 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 1400–1409, Austin, Texas. Asso-\\nciation for Computational Linguistics.\\nGeorge A. Miller. 1995. Wordnet: A lexical database\\nfor english. Commun. ACM , 38(11):39–41.\\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\\nStan Peshterliev, Dmytro Okhonko, Michael\\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\\nScott Yih. 2020. UniK-QA: Uniﬁed representations\\nof structured and unstructured knowledge for\\nopen-domain question answering. arXiv preprint\\narXiv:2012.14610 .\\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\\nSmith. 2019. Knowledge enhanced contextual word\\nrepresentations. In Proceedings of the 2019 Con-\\nference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-\\nIJCNLP) , pages 43–54, Hong Kong, China. Associ-\\nation for Computational Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2019. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. CoRR , abs/1910.10683.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 5418–5426,\\nOnline. Association for Computational Linguistics.\\nSebastian Ruder, Matthew E. Peters, Swabha\\nSwayamdipta, and Thomas Wolf. 2019. Trans-\\nfer learning in natural language processing. In\\nProceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Com-\\nputational Linguistics: Tutorials , pages 15–18,\\nMinneapolis, Minnesota. Association for Computa-\\ntional Linguistics.\\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\\nAtomic: An atlas of machine commonsense for if-\\nthen reasoning. In Proceedings of the AAAI Confer-\\nence on Artiﬁcial Intelligence , volume 33.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nCoRR , abs/1804.04235.Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.\\nConceptnet 5.5: An open multilingual graph of gen-\\neral knowledge. In Thirty-ﬁrst AAAI conference on\\nartiﬁcial intelligence .\\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi\\nChen, Yanbin Zhao, Yuxiang Lu, et al. 2021. Ernie\\n3.0: Large-scale knowledge enhanced pre-training\\nfor language understanding and generation. arXiv\\npreprint arXiv:2107.02137 .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems , volume 30. Curran Associates, Inc.\\nPat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Cohen. 2021. Adaptable and interpretable\\nneural MemoryOver symbolic knowledge. In Pro-\\nceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n3678–3691, Online. Association for Computational\\nLinguistics.\\nDenny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-\\ndata: A free collaborative knowledgebase. Commu-\\nnications of the ACM , 57(10):78–85.\\nJohannes Welbl, Pontus Stenetorp, and Sebastian\\nRiedel. 2018. Constructing datasets for multi-hop\\nreading comprehension across documents. Transac-\\ntions of the Association for Computational Linguis-\\ntics, 6:287–302.\\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\\ning Yang, and Michael Zeng. 2021. Kg-ﬁd: In-\\nfusing knowledge graph in fusion-in-decoder for\\nopen-domain question answering. arXiv preprint\\narXiv:2110.04330 .\\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\\nder J Smola, and Le Song. 2018. Variational reason-\\ning for question answering with knowledge graph.\\nInAAAI .\\nA Matching of entities in KELM\\nsentences\\nTo ﬁnd Wikidata KG entities in corresponding\\nKELM sentences, we use Algorithm 1. Additional\\ncycle on line 22 is needed because some entities\\nhave an information in brackets that should not\\nbe in a sentence, for example John Doe (born\\n1990) . This algorithm matched at least one en-\\ntity to 15,383,248out of 15,628,486KELM sen-\\ntences.\\nWe don’t try to match relation part of triples,\\nbecause it could be represented in many different', metadata={'source': './docs/KG-LLM.pdf', 'page': 6}),\n",
              " Document(page_content='Algorithm 1 KELM-Wikidata matching algorithm\\nthat ﬁnds spans in KELM sentences corresponding\\nto Wikidata KG entities. a⊂bmeans that ais a\\nsubstring of b.∗represents any string.\\n1:KELM matched←∅\\n2:for each k∈KELM sentences do\\n3: for each t∈triples (k)do\\n4: for each e∈entities (t)do\\n5: ep←PREPROCESS (e)\\n6: kp←PREPROCESS (k)\\n7: spans←MATCH ENTITY (ep,kp)\\n8: KELM matched .insert ([k, spans ])\\n9: end for\\n10: end for\\n11:end for\\n12:\\n13:function MATCH ENTITY (e: entity, k: KELM\\nsentence)\\n14: spans←∅\\n15: for each s⊂k:date(e) =date(s)do\\n16: spans.insert (s)\\n17: end for\\n18: for each∃s⊂k:e=sdo\\n19: spans.insert (s)\\n20: end for\\n21: ifspans =∅then\\n22: for each∃s⊂k:e=s+\" (*)\" do\\n23: spans.insert (s)\\n24: end for\\n25: end if\\n26: return spans\\n27:end function\\n28:\\n29:function PREPROCESS (str: string)\\n30: str←Lowercase (str)\\n31: str←RemovePunctuation (str)\\n32: return str\\n33:end functionforms. For example, the triple ( Pulp Fiction ,\\ncast member ,John Travolta ) could\\nbe represented as \" John Travolta was\\nan actor in Pulp Fiction \", \" John\\nTravolta starred in Pulp Fiction \",\\n\"John Travolta played Vincent\\nVega in Pulp Fiction \", etc., and there is\\nno way to robustly align a relation to all possible\\nsurface forms.\\nB Dataset\\nWikidata (Vrande ˇci´c and Krötzsch, 2014) was re-\\nleased under the Creative Commons CC0 License.\\nKELM (Agarwal et al., 2021) was released under\\nthe Creative Commons CC BY-SA 2.0 License.\\nNaturalQuestions (Kwiatkowski et al., 2019) and\\nWikiHop (Welbl et al., 2018) were released un-\\nder Creative Commons CC BY-SA 3.0 License.\\nMetaQA (Zhang et al., 2018) was released under\\nCreative Commons CC BY-ND 3.0 License. C4\\n(Raffel et al., 2019) and TriviaQA (Joshi et al.,\\n2017) were released under Apache-2.0 License.\\nWikiMovies (Miller et al., 2016) was released un-\\nder MIT License. FreebaseQA (Jiang et al., 2019)4\\nwas released without a license.\\ntrain dev test\\nFreebaseQA 20,358 3 ,994 3 ,996\\nWikiHop 39,364 4 ,374 5 ,129\\nTQA 78,785 8 ,837 11 ,313\\nTQA-matched 20,948 2 ,289 3 ,064\\nNQ 79,168 8 ,757 3 ,610\\nNQ-matched 10,487 1 ,160 462\\nMetaQA-1hop 96,106 9 ,992 9 ,947\\nMetaQA-2hop 118 ,980 14 ,872 14 ,872\\nMetaQA-3hop 114 ,196 14 ,274 14 ,274\\nTable 4: Dataset sizes for the closed-book QA tasks.\\nTQA and NQ stands for TriviaQA and NaturalQues-\\ntions, respectively. *-matched are the selected dataset\\nwith the Wikidata KG answerable questions, and the\\nKG alignment details can be found in Section 4.1.\\n4https://github.com/kelvin-jiang/\\nFreebaseQA', metadata={'source': './docs/KG-LLM.pdf', 'page': 7}),\n",
              " Document(page_content='Distilling Step-by-Step! Outperforming Larger Language Models\\nwith Less Training Data and Smaller Model Sizes\\nCheng-Yu Hsieh1∗, Chun-Liang Li2, Chih-Kuan Yeh3, Hootan Nakhost2,\\nYasuhisa Fujii3, Alexander Ratner1, Ranjay Krishna1, Chen-Yu Lee2, Tomas Pfister2\\n1University of Washington,2Google Cloud AI Research,3Google Research\\ncydhsieh@cs.washington.edu\\nAbstract\\nDeploying large language models (LLMs) is\\nchallenging because they are memory inef-\\nficient and compute-intensive for practical\\napplications. In reaction, researchers train\\nsmaller task-specific models by either finetun-\\ning with human labels or distilling using LLM-\\ngenerated labels. However, finetuning and dis-\\ntillation require large amounts of training data\\nto achieve comparable performance to LLMs.\\nWe introduce Distilling step-by-step , a new\\nmechanism that (a) trains smaller models that\\noutperform LLMs, and (b) achieves so by lever-\\naging less training data needed by finetuning\\nor distillation. Our method extracts LLM ra-\\ntionales as additional supervision for training\\nsmall models within a multi-task framework.\\nWe present three findings across 4NLP bench-\\nmarks: First, compared to both finetuning and\\ndistillation, our mechanism achieves better per-\\nformance with much fewer labeled/unlabeled\\ntraining examples. Second, compared to few-\\nshot prompted LLMs, we achieve better perfor-\\nmance using substantially smaller model sizes.\\nThird, we reduce both the model size and the\\namount of data required to outperform LLMs;\\nour finetuned 770M T5 model outperforms the\\nfew-shot prompted 540B PaLM model using\\nonly80% of available data on a benchmark,\\nwhereas standard finetuning the same T5 model\\nstruggles to match even by using 100% of the\\ndataset.1\\n1 Introduction\\nDespite the impressive few-shot ability offered by\\nlarge language models (LLMs) (Brown et al., 2020;\\nChowdhery et al., 2022; Thoppilan et al., 2022;\\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\\net al., 2022), these models are challenging to de-\\nploy in real world applications due to their sheer\\n∗Work done while the author was a student researcher at\\nGoogle Cloud AI Research.\\n1Source code is available at: https://github.com/\\ngoogle-research/distilling-step-by-step .\\nFigure 1: While large language models (LLMs) offer\\nstrong zero/few-shot performance, they are challenging\\nto serve in practice. Traditional ways of training small\\ntask-specific models, on the other hand, requires large\\namount of training data. We propose Distilling step-\\nby-step, a new paradigm that extracts rationales from\\nLLMs as informative task knowledge into training small\\nmodels, which reduces both the deployed model size as\\nwell as the data required for training.\\nsize. Serving a single 175billion LLM requires\\nat least 350GB GPU memory using specialized in-\\nfrastructure (Zheng et al., 2022). To make matters\\nworse, today’s state-of-the-art LLMs are composed\\nof over 500B parameters (Chowdhery et al., 2022),\\nrequiring significantly more memory and compute.\\nSuch computational requirements are far beyond\\naffordable for most product teams, especially for\\napplications that require low latency performance.\\nTo circumvent these deployment challenges of\\nlarge models, practitioners often choose to de-\\nploy smaller specialized models instead. These\\nsmaller models are trained using one of two\\ncommon paradigms: finetuning ordistillation .\\nFinetuning updates a pretrained smaller model\\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\\net al., 2020)) using downstream human annotated\\ndata (Howard and Ruder, 2018). Distillation trains\\nthe same smaller models with labels generated by\\na larger LLM (Tang et al., 2019; Wang et al., 2021;\\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\\nnately, these paradigms reduce model size at a cost:\\nto achieve comparable performance to LLMs, fine-\\ntuning requires expensive human labels, and dis-arXiv:2305.02301v2  [cs.CL]  5 Jul 2023', metadata={'source': './docs/distilling-LLM.pdf', 'page': 0}),\n",
              " Document(page_content='tillation requires large amounts of unlabeled data\\nwhich can be hard to obtain (Tang et al., 2019;\\nLiang et al., 2020).\\nIn this work, we introduce Distilling step-by-\\nstep, a new simple mechanism for training smaller\\nmodels with less training data. Our mechanism re-\\nduces the amount of training data required for both\\nfinetuning and distillation of LLMs into smaller\\nmodel sizes. Core to our mechanism is changing\\nour perspective from viewing LLMs as a source\\nof noisy labels to viewing them as agents that can\\nreason: LLMs can produce natural language ratio-\\nnales justifying their predicted labels (Wei et al.,\\n2022; Kojima et al., 2022). For example, when\\nasked “ Jesse’s room is 11feet long and 15feet\\nwide. If she already has 16square feet of carpet.\\nHow much more carpet does she need to cover\\nthe whole floor? ”, an LLM can be prompted by\\nchain-of-thought (CoT) technique (Wei et al., 2022)\\nto provide intermediate rationales “ Area=length\\n×width. Jesse’s room has 11×15square feet. ”\\nthat better connects the input to the final answer\\n“(11×15)−16”. These rationales can contain\\nrelevant task knowledge, such as “ Area=length×\\nwidth ”, that may originally require many data for\\nsmall task-specific models to learn. We thus utilize\\nthese extracted rationales as additional, richer infor-\\nmation to train small models through a multi-task\\ntraining setup, with both label prediction and ratio-\\nnale prediction tasks (Raffel et al., 2020; Narang\\net al., 2020).\\nDistilling step-by-step allows us to learn task-\\nspecific smaller models that outperform LLMs us-\\ning over 500×less model parameters, and it does\\nso with far fewer training examples compared to\\ntraditional finetuning or distillation (Figure 1). Our\\nresults show three promising empirical conclusions\\nacross 4NLP benchmarks. First, compared to both\\nfinetuning and distillation, our resulting models\\nachieve better performance with over 50% less\\ntraining examples on average across datasets (and\\nup to over 85% reduction). Second, our models\\noutperform LLMs with much smaller model sizes\\n(up to 2000×smaller), drastically reducing the\\ncomputation cost required for model deployment.\\nThird, we simultaneously reduce the model size\\nas well as the amount of data required to outper-\\nform LLMs. We surpass the performance of 540B\\nparameter LLMs using a 770M T5 model; this\\nsmaller model only uses 80% of a labeled dataset\\nthat would otherwise be required if using an exist-ing finetuning method. When only unlabeled data\\nis present, our small models still perform on par or\\nbetter than LLMs. We outperform 540B PaLM’s\\nperformance with only a 11B T5 model. We further\\nshow that when a smaller model performs worse\\nthan an LLM, Distilling step-by-step can more effi-\\nciently leverage additional unlabeled data to match\\nthe LLM performance compared to the standard\\ndistillation approach.\\n2 Related work\\nOur work distills task-specific knowledge of LLMs\\ninto smaller specialist models by leveraging the\\nemergent reasoning capabilities of today’s LLMs.\\nWe draw on knowledge distillation research and\\nmethods that learn from both human-generated ra-\\ntionales and LLM-generated rationales.\\nKnowledge distillation from large models.\\nKnowledge distillation has been successfully used\\nto transfer knowledge from larger, more compe-\\ntent teacher models into smaller student models\\naffordable for practical applications (Bucilu ˇa et al.,\\n2006; Hinton et al., 2015; Beyer et al., 2022; West\\net al., 2021; Fu et al., 2023). It supports learning\\nfrom limited labeled data, since the larger teacher\\nmodel is often used to generate a training dataset\\nwith noisy pseudo labels (Chen et al., 2020; Il-\\niopoulos et al., 2022; Wang et al., 2021; Smith\\net al., 2022a; Arora et al., 2022; Agrawal et al.,\\n2022). The one limitation that knowledge distil-\\nlation often faces is its reliance on large amounts\\nof unlabelled data required to create a useful noisy\\ntraining dataset. Although prior work has explored\\nusing data augmentation techniques to reduce this\\nhunger for data (Tang et al., 2019; Liang et al.,\\n2020; Srinivas and Fleuret, 2018; Milli et al., 2019),\\nwe propose an alternative approach: we reduce the\\nneed for large unlabeled data by distilling not just\\nlabels but also the teacher’s rationales.\\nLearning with human rationales. While utiliz-\\ning LLM-generated rationales is a new exciting\\narea of investigation, using human-generated ratio-\\nnales has a rich history (Hase and Bansal, 2021).\\nFor instance, human rationales can be used to reg-\\nularize model behavior (Ross et al., 2017); it can\\nbe used as additional inputs to guide a model’s\\npredictions (Rajani et al., 2019); it can be used to\\nimprove overall model performance (Zaidan et al.,\\n2007; Zhang et al., 2016; Camburu et al., 2018;\\nHancock et al., 2019; Pruthi et al., 2022); and hu-', metadata={'source': './docs/distilling-LLM.pdf', 'page': 1}),\n",
              " Document(page_content='Figure 2: Overview on Distilling step-by-step. We first utilize CoT prompting to extract rationales from an LLM\\n(Section 3.1). We then use the generated rationales to train small task-specific models within a multi-task learning\\nframework where we prepend task prefixes to the input examples and train the model to output differently based on\\nthe given task prefix (Section 3.2).\\nman rationales can be used as gold standard labels\\nto make models more interpretable by generating\\nsimilar rationales (Wiegreffe et al., 2021; Narang\\net al., 2020; Eisenstein et al., 2022). Unfortunately,\\nhuman rationales are expensive.\\nLearning with LLM generated rationales. To-\\nday’s LLMs are capable of explaining their pre-\\ndictions by generating high-quality reasoning\\nsteps (Wei et al., 2022; Kojima et al., 2022). These\\nreasoning steps have been used to augment input\\nprompts to LLMs, improving their few-shot or zero-\\nshot performance (Wei et al., 2022; Kojima et al.,\\n2022; Wang et al., 2022b); reasoning steps have\\nalso been used as additional finetuning data “self-\\nimprove” LLMs (Zelikman et al., 2022; Huang\\net al., 2022). Unfortunately, regardless of how\\nLLMs are improved, their large size limits their\\nutility in most test-time applications.\\nBy contrast, we leverage generated rationales\\nas informative supervision to train smaller task-\\nspecific models, i.e. models that can be deployed\\nwithout incurring large computation or memory\\ncosts. Several concurrent works have also proposed\\na similar idea to ours – that of using extracted ra-\\ntionales as supervision (Wang et al., 2022a; Ho\\net al., 2022; Magister et al., 2022; Li et al., 2023).\\nAmongst them, PINTO (Wang et al., 2022a) relies\\non an LLM to generate rationales at test-time, and\\nthus does not fully solve deployment challenges.\\nCompared with Ho et al. (2022) and Magister et al.\\n(2022), we go beyond their experiments to provide\\na granular study by varying training dataset size,\\nexploring downstream model sizes, and demon-strating the effectiveness of our method on fully\\nunlabeled datasets.\\n3 Distilling step-by-step\\nWe propose a new paradigm, Distilling step-by-\\nstep, that leverages the ability of LLMs to reason\\nabout their predictions to train smaller models in\\na data-efficient way. Our overall framework is il-\\nlustrated in Figure 2. Our paradigm has two sim-\\nple steps: First, given an LLM and an unlabeled\\ndataset, we prompt the LLM to generate output\\nlabels along with rationales to justify the labels.\\nRationales are natural language explanations that\\nprovide support for the model’s predicted label\\n(see Figure 2). Second, we leverage these ratio-\\nnales in addition to the task labels to train smaller\\ndownstream models. Intuitively, rationales provide\\nricher, more detailed information about why an in-\\nput is mapped to a specific output label, and often\\ncontain relevant task knowledge that may be hard\\nto infer solely from the original inputs.\\n3.1 Extracting rationales from LLMs\\nRecent studies observe one intriguing emerging\\nproperty of LLMs: their ability to generate ra-\\ntionales that support their predictions (Wei et al.,\\n2022; Kojima et al., 2022). While the studies have\\nlargely focused on how to elicit such reasoning ca-\\npability from LLMs (Nye et al., 2021; Wei et al.,\\n2022; Kojima et al., 2022), we use them in training\\nsmaller downstream models.\\nSpecifically, we utilize Chain-of-Thought (CoT)\\nprompting (Wei et al., 2022) to elicit and extract', metadata={'source': './docs/distilling-LLM.pdf', 'page': 2}),\n",
              " Document(page_content='Figure 3: We use few-shot CoT prompting that contains\\nboth an example rationale (highlighted in green) and a\\nlabel (highlighted in blue) to elicit rationales from an\\nLLM on new input examples.\\nrationales from LLMs. As illustrated in Figure 3,\\ngiven an unlabeled dataset xi∈D, we first cu-\\nrate a prompt template pthat articulates how the\\ntask should be solved. Each prompt is a triplet\\n(xp, rp, yp), where xpis an example input, ypis\\nits corresponding label and rpis a user-provided\\nrationale that explains why xpcan be categorized\\nasyp. We append each input xitopand use it as\\nan input to prompt the LLM to generate rationales\\nand labels for each xi∈D. With the demonstra-\\ntions seen in p, the LLM is able to mimic the triplet\\ndemonstration to generate the rationale ˆriand out-\\nputˆyiforxi.\\n3.2 Training smaller models with rationales\\nWe first describe the current framework for learn-\\ning task-specific models. With this framework in\\nplace, we extend it to incorporate rationales into\\nthe training process. Formally, we denote a dataset\\nasD={(xi, yi)}N\\ni=1where each xirepresents an\\ninput and yiis the corresponding desired output\\nlabel. While our framework supports inputs and\\noutputs of any modality, our experiments limits\\nxandyto be natural language. This text-to-text\\nframework (Raffel et al., 2020) encompasses a va-\\nriety of NLP tasks: classification, natural language\\ninference, question answering and more.\\nStandard finetuning and task distillation. The\\nmost common practice to train a task-specific\\nmodel is to finetune a pretrained model with su-\\npervised data (Howard and Ruder, 2018). In the\\nabsence of human-annotated labels, task-specific\\ndistillation (Hinton et al., 2015; Tang et al., 2019)\\nuses LLM teachers to generates pseudo noisy train-\\ning labels, ˆyiin place of yi(Wang et al., 2021;\\nSmith et al., 2022a; Arora et al., 2022).\\nFor both scenarios, the smaller model fistrained to minimize the label prediction loss:\\nLlabel=1\\nNNX\\ni=1ℓ(f(xi),ˆyi), (1)\\nwhere ℓis the cross-entropy loss between the pre-\\ndicted and target tokens. Note that for ease of\\nexposition, we overload ˆyiin Eq. 1 to be either\\nhuman-annotated labels yifor the standard finetun-\\ning case, or LLM-predicted labels ˆyifor the model\\ndistillation case.\\nMulti-task learning with rationales. To create\\na more explicit connection between xi’s toˆyi’s, we\\nuse extracted rationales ˆrias additional supervi-\\nsion. There are several ways to incorporate ratio-\\nnales into the downstream model’s training process.\\nOne straightforward approach is feed ˆrias an ad-\\nditional input—as proposed by other concurrent\\nresearch (Rajani et al., 2019; Wang et al., 2022a).\\nIn other words, the f(xi,ˆri)→ˆyiis trained with\\nboth text and rationale [x, r]as inputs:\\nL=1\\nNNX\\ni=1ℓ(f(xi,ˆri),ˆyi). (2)\\nUnfortunately, this design requires an LLM to first\\ngenerate a rationale before the fcan make a pre-\\ndiction. The LLM is still necessary during deploy-\\nment, limited its deployability.\\nIn this work, instead of using rationales as ad-\\nditional model inputs, we frame learning with ra-\\ntionales as a multi-task problem. Specifically, we\\ntrain the model f(xi)→(ˆyi,ˆri)to not only predict\\nthe task labels but also generate the corresponding\\nrationales given the text inputs:\\nL=Llabel+λLrationale , (3)\\nwhere Llabel is the label prediction loss in Eq. 1\\nandLrationale is the rationale generation loss :\\nLrationale =1\\nNNX\\ni=1ℓ(f(xi),ˆri). (4)\\nThe rationale generation loss enables the model to\\nlearn to generate the intermediate reasoning steps\\nfor the prediction, and could therefore guide the\\nmodel in better predicting the resultant label. This\\nis our proposed Distilling step-by-step. Compared\\nwith Eq. 2, the rationale ˆriis not required in the\\ntest time, which removes the need for an LLM at\\ntest-time.', metadata={'source': './docs/distilling-LLM.pdf', 'page': 3}),\n",
              " Document(page_content='We prepend “task prefixes” ( [label] ,\\n[rationale] ) to the input examples and\\ntrain the smaller model to output ˆyiwhen\\n[label] is provided and to produce ˆriwith\\n[rationale] (Raffel et al., 2020).\\n4 Experiments\\nWe empirically validate the effectiveness of Dis-\\ntilling step-by-step. First, we show that when\\ncompared to standard finetuning and task distil-\\nlation approaches, Distilling step-by-step achieves\\nbetter performance with much fewer number of\\ntraining examples, substantially improving the\\ndata efficiency to learn small task-specific mod-\\nels (Sec. 4.1). Second, we show that Distilling\\nstep-by-step surpasses the performance of LLMs\\nwith much smaller model size, drastically lowering\\nthe deployment cost compared to LLMs (Sec. 4.2).\\nThird, we investigate the minimum resources re-\\nquired, w.r.t. both number of training examples and\\nmodel size, for Distilling step-by-step to outper-\\nform LLMs. We show that Distilling step-by-step\\noutperforms LLMs by using less data and smaller\\nmodel, simultaneously improving both data- and\\ndeployability-efficiency (Sec. 4.3). Finally, we per-\\nform ablation studies to understand the influence\\nof different components and design choices in the\\nDistilling step-by-step framework (Sec. 4.4).\\nSetup. In the experiments, we consider the 540B\\nPaLM model (Chowdhery et al., 2022) as the LLM.\\nFor task-specific downstream models, we use T5\\nmodels (Raffel et al., 2020) where we initialize the\\nmodels with pretrained weights obtained from pub-\\nlicly available sources2. For CoT prompting, we\\nfollow Wei et al. (2022) when available, and curate\\nour own examples for new datasets. We include\\nmore implementation details in Appendix A.1.\\nDatasets. We conduct the experiments on 4\\npopular benchmark datasets across 3 different\\nNLP tasks: e-SNLI (Camburu et al., 2018) and\\nANLI (Nie et al., 2020) for natural language infer-\\nence; CQA (Talmor et al., 2019; Rajani et al., 2019)\\nfor commonsense question answering; SVAMP (Pa-\\ntel et al., 2021) for arithmetic math word problems.\\nWe include more dataset details in Appendix A.2.\\n4.1 Reducing training data\\nWe compare Distilling step-by-step to two most\\ncommon methods in learning task-specific models:\\n2https://huggingface.co/(1)STANDARD FINETUNING when human-labeled\\nexamples are available, and (2) STANDARD TASK\\nDISTILLATION when only unlabeled examples are\\navailable. Specifically, standard finetuning refers to\\nthe prevailing pretrain-then-finetune paradigm that\\nfinetunes a model with ground-truth labels via stan-\\ndard label supervision (Howard and Ruder, 2018).\\nOn the other hand, when only unlabeled examples\\nare available, standard task distillation learns the\\ntask-specific model by treating a teacher LLM’s\\npredicted labels as ground-truths (Hinton et al.,\\n2015; Chen et al., 2020; Wang et al., 2021; Smith\\net al., 2022a; Arora et al., 2022).\\nIn the following set of experiments, we fix the\\ntask-specific models to be 220M T5-Base models,\\nand compare the task performances achieved by dif-\\nferent methods under varying number of available\\ntraining examples.\\nDistilling step-by-step outperforms standard\\nfinetuning with much less labeled examples.\\nWhen finetuned with human-labeled examples, Fig-\\nure 4 shows that Distilling step-by-step consistently\\nachieves better performance than standard finetun-\\ning across varying numbers of labeled examples\\nused. Furthermore, we see that Distilling step-by-\\nstep can achieve the same performance as stan-\\ndard finetuning with much less labeled examples.\\nIn particular, by using only 12.5%of the full e-\\nSNLI dataset, Distilling step-by-step can outper-\\nform standard finetuning trained with 100% of the\\nfull dataset. Similarly, we achieve 75%,25%, and\\n20% reduction in training examples required to out-\\nperform standard finetuning on ANLI, CQA, and\\nSV AMP respectively.\\nDistilling step-by-step outperforms standard dis-\\ntillation with much less unlabeled examples.\\nWhen only unlabeled data is available, we compare\\nDistilling step-by-step to standard task distillation.\\nIn Figure 5, we observe an overall similar trend to\\nthe finetuning setup. Specifically, we see that Dis-\\ntilling step-by-step outperforms standard task distil-\\nlation on all 4datasets under different numbers of\\nunlabeled data used. We as well see that Distilling\\nstep-by-step requires much less unlabeled data to\\noutperform standard task distillation. For instance,\\nwe need only 12.5%of the full unlabeled dataset\\nto outperform the performance achieved by stan-\\ndard task distillation using 100% of the training\\nexamples on e-SNLI dataset.', metadata={'source': './docs/distilling-LLM.pdf', 'page': 4}),\n",
              " Document(page_content='Figure 4: We compare Distilling step-by-step and Standard finetuning using 220M T5 models on varying sizes of\\nhuman-labeled datasets. On all datasets, Distilling step-by-step is able to outperform Standard finetuning, trained on\\nthe full dataset, by using much less training examples (e.g., 12.5%of the full e-SNLI dataset).\\nFigure 5: Similar to the plots above, we compare Distilling step-by-step and Standard task distillation using 220M\\nT5 models on varying sizes of unlabeled datasets. Distilling step-by-step is able to outperform Standard task\\ndistillation by using only a small subset of the full unlabeled dataset (e.g., 12.5%on ANLI dataset).\\n4.2 Reducing model size\\nIn the following set of experiments, we hold the\\ntraining set size fixed (using 100% of the datasets),\\nand compare varying sizes of small T5 models\\ntrained with Distilling step-by-step and standard\\napproaches to LLMs. Specifically, we consider 3\\ndifferent sizes of T5 models, i.e., 220M T5-Base,\\n770M T5-Large, and 11B T5-XXL. For LLMs,\\nwe include two baseline methods: (1) FEW-SHOT\\nCOT(Wei et al., 2022), and (2) PINTO TUN-\\nING(Wang et al., 2022a). Few-shot CoT directly\\nutilizes CoT demonstrations to prompt the 540B\\nPaLM to generate intermediate steps before pre-\\ndicting the final labels without any further fine-\\ntuning of the LLM. PINTO tuning refers to our\\nextension of Wang et al. (2022a) to handle tasks\\nbeyond question-answering, which are not stud-\\nied by Wang et al. (2022a). Here, we finetune a\\n220M T5-Base model on top of the outputs gener-\\nated from the PaLM model, which can be viewed\\nas a finetuning method for LLMs with additional\\nparameters (Zhang et al., 2020; Lester et al., 2021).\\nWe present the experimental results under thetwo broad scenarios of having access to labeled\\ndatasets or unlabeled datasets in Figure 6 and Fig-\\nure 7, respectively. We plot each method by their\\ndeployed model sizes for prediction ( x-axis), and\\ntheir corresponding task performances ( y-axis).\\nDistilling step-by-step improves over standard\\nbaselines across varying model sizes used. In\\nFigure 6 and Figure 7 respectively, we see that\\nDistilling step-by-step consistently improves over\\nstandard finetuning and standard distillation across\\nall sizes of T5 models. The improvements are most\\npronounced on ANLI, where Distilling step-by-\\nstep outperforms standard finetuning and distilla-\\ntion by an average of 8%and13% on task accuracy\\nrespectively.\\nDistilling step-by-step outperforms LLMs by\\nusing much smaller task-specific models. In\\nFigure 6 when human-labeled datasets are avail-\\nable, Distilling step-by-step can always outper-\\nform Few-shot CoT and PINTO tuning on all 4\\ndatasets considered, by using much smaller T5\\nmodels. For instance, we can achieve better perfor-\\nmances than 540B PaLM model’s Few-shot CoT', metadata={'source': './docs/distilling-LLM.pdf', 'page': 5}),\n",
              " Document(page_content='Figure 6: We perform Distilling step-by-step and Standard finetuning, using the full human-labeled datasets, on\\nvarying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO\\nTuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700×\\nsmaller model on ANLI. Standard finetuning fails to match LLM’s performance using the same model size.\\nFigure 7: Using unlabeled datasets, we perform Distilling step-by-step and Standard task distillation on varying\\nsizes of T5 models and compare them to Few-shot CoT. Distilling step-by-step outperforms Few-shot CoT by using\\n2000×smaller models on e-SNLI and 45×smaller models on ANLI and CQA. On SV AMP, by adding unlabeled\\nexamples from ASDiv, we close the gap to Few-shot CoT whereas Standard distillation still struggles to catch up.\\nwith220M (over 2000×smaller) T5 model on e-\\nSNLI, 770M (over 700×smaller) T5 models on\\nANLI and SV AMP, and 11B (over 45×smaller)\\nT5 model on CQA. These results hold true even\\nby further finetuning the 540B PaLM model on\\navailable labeled data with PINTO tuning3.\\nIn Figure 7, by only utilizing unlabeled exam-\\nples, Distilling step-by-step also outperforms the\\nteacher LLM on 3 out of 4 datasets. Specifically,\\nDistilling step-by-step surpasses the 540B PaLM\\nmodel’s Few-shot CoT performance by using 11B\\nT5 with less than 3%of PaLM’s size. On SV AMP\\nwhere the distilled model underperforms, we hy-\\npothesize that the performance gap is due to the\\nrelatively small number of data points in the dataset\\n(i.e.,800). In reaction, we propose to augment the\\ndataset with additional unlabeled examples to close\\nthe performance gap as shown in next.\\n3We note that PETuning methods may outperform PINTO\\ntuning. However, they require massive resource in both train-\\ning and deployment, which is not the focus of this work.Unlabeled data augmentation further improves\\nDistilling step-by-step. We augment the SV AMP\\ntraining set with unlabeled examples from the AS-\\nDivdataset (Miao et al., 2020). ASDiv dataset\\ncontains a total of 2,305examples, where each ex-\\nample is a math word problem similar to the ones in\\nSV AMP. In Figure 7 on SV AMP, we show the per-\\nformances of Distilling step-by-step and standard\\ntask distillation using 11B T5 model after augment-\\ning the training set with ASDiv. We see the data\\naugmentation much improves the performance for\\nboth Distilling step-by-step and standard task dis-\\ntillation. However, even with the added unlabeled\\nexamples, standard task distillation still underper-\\nforms Few-shot CoT. On the other hand, Distilling\\nstep-by-step is able to much more efficiently ex-\\nploit the value of the added examples to achieve the\\nsame performance level of Few-shot CoT, again,\\nusing a T5 model of size less than 3%of the 540B\\nPaLM.', metadata={'source': './docs/distilling-LLM.pdf', 'page': 6}),\n",
              " Document(page_content='Figure 8: We show the minimum size of T5 models and the least amount of human-labeled examples required\\nfor Distilling step-by-step to outperform LLM’s Few-shot CoT by a coarse-grained search. Distilling step-by-step\\nis able to outperform Few-shot CoT using not only much smaller models, but it also achieves so with much less\\ntraining examples compared to Standard finetuning. On ANLI, we outperform the LLM CoT with a 770M model\\nusing only 80% of the dataset, whereas Standard finetuning struggles to match even using 100% of the dataset.\\nFigure 9: Similar to Figure 8 but using only unlabeled examples, Distilling step-by-step is able to outperform\\nFew-shot CoT using much smaller models and with much less examples compared to Standard task distillation. On\\nSV AMP, the x-axis corresponds to the size of ASDiv dataset used for augmenting the original SV AMP dataset, i.e.,\\nx= 0is without augmentation and x= 100 corresponds to adding the full ASDiv dataset.\\n4.3 Outperforming LLMs using minimum\\nmodel size and least training data\\nHere, using the LLM’s performance as an anchor\\npoint, we explore the most efficient resource re-\\nquirements in terms of both number of training\\nexamples and deployed model size, that Distill-\\ning step-by-step and standard finetuning/distillation\\nneed to outperform the LLM. We present the re-\\nsults, again under human-labeled setting and unla-\\nbeled setting, in Figure 8 and Figure 9 respectively.\\nWe visualize the results by plotting different resul-\\ntant models by (1) the number of training exam-\\nples used ( x-axis), (2) the final task performance\\nachieved ( y-axis), and (3) the size of the model\\n(visualized by the size of the shaded area).\\nDistilling step-by-step outperforms LLMs with\\nmuch smaller models by using less data. On\\nall datasets in Figure 8, we see that Distilling step-\\nby-step outperforms PaLM’s Few-shot CoT with\\nmuch smaller T5 models using only a subset of\\nthe available training examples. Specifically, on\\ne-SNLI, Distilling step-by-step can achieve bet-ter performance than Few-shot CoT with a model\\nover2000×smaller (220M T5) and only 0.1%of\\nthe full dataset. In Figure 9 where only unlabeled\\ndatasets are available, we observe the same trend\\nthat Distilling step-by-step can, at most time, out-\\nperform Few-shot CoT with smaller model as well\\nas less data. For instance, on ANLI, Distilling step-\\nby-step outperforms the LLM with a 45×smaller\\nmodel and 50% of the full unlabeled set.\\nStandard finetuning and distillation require\\nmore data and larger model. Finally, in Fig-\\nure 8 and Figure 9, we see that standard finetuning\\nand distillation often need either more data or larger\\nmodels to match LLM’s performance. For instance,\\non e-SNLI in Figure 8, we observe that Distilling\\nstep-by-step outperform the LLM using only 0.1%\\nof the dataset while standard finetuning requires\\nmore data to match the performance. Furthermore,\\non ANLI in Figure 8, we observe that Distilling\\nstep-by-step can outperform PaLM using 770M\\nmodel with only 80% of the training set while stan-\\ndard finetuning struggles to match the LLM even', metadata={'source': './docs/distilling-LLM.pdf', 'page': 7}),\n",
              " Document(page_content='Table 1: Distilling step-by-step works with different\\nsizes of LLMs. When rationales are extracted from a\\n20B GPT-NeoX model, Distilling step-by-step is still\\nable to provide performance lift compared to standard\\nfinetuning on 220M T5 models.\\nDataset\\nMethod LLM e-SNLI ANLI CQA SV AMP\\nSTANDARD FINETUNING N/A 88.38 43.58 62.19 62.63\\nDISTILLING STEP -BY-STEP 20B 89.12 48.15 63.25 63.00\\nDISTILLING STEP -BY-STEP 540B 89.51 49.58 63.29 65.50\\nusing the full dataset and thus requires larger model\\nto close the performance gap.\\n4.4 Further ablation studies\\nSo far, we have focused on showing the effective-\\nness of Distilling step-by-step on reducing the train-\\ning data required for finetuning or distilling smaller\\ntask-specific models. In this section, we perform\\nfurther studies to understand the influence of dif-\\nferent components in the Distilling step-by-step\\nframework. Specifically, we study (1) how differ-\\nent LLMs, from which the rationales are extracted,\\naffect the effectiveness of Distilling step-by-step,\\nand (2) how the multi-task training approach com-\\npares to other potential design choices in training\\nsmall task-specific models with LLM rationales.\\nHere, we fix the small task-specific models to be\\n220M T5 models, and utilize 100% of the data on\\nall datasets.\\nDistilling step-by-step works with different sizes\\nof decently trained LLMs. In addition to using\\n540B PaLM as the LLM, here we consider a rela-\\ntively smaller LLM, 20B GPT-NeoX model (Black\\net al., 2022), from which we extract rationales for\\nDistilling step-by-step. In Table 1, we see that\\nwhen coupled with LLMs of different sizes, Distill-\\ning step-by-step can still provide performance im-\\nprovements compared to standard finetuning. How-\\never, the performance lift is smaller when rationales\\nare extracted from the 20B GPT-NeoX model in-\\nstead of from the 540B PaLM. This can be due\\nto the fact that the larger PaLM model provides\\nhigher-quality rationales that are more beneficial\\nfor learning the task.\\nMulti-task training is much more effective than\\nsingle-task rationale and label joint prediction.\\nThere are different possible ways to train task-\\nspecific models with LLM-rationales as output su-\\npervisions. One straightforward approach is to con-\\ncatenate the rationale ˆriand label ˆyiinto a singleTable 2: Our proposed multi-task training framework\\nconsistently leads to better performances than treating\\nrationale and label predictions as a single task. Single-\\ntask training can at times lead to worse performance\\nthan standard finetuning.\\nDataset\\nMethod e-SNLI ANLI CQA SV AMP\\nSTANDARD FINETUNING 88.38 43.58 62.19 62.63\\nSINGLE -TASK TRAINING 88.88 43.50 61.37 63.00\\nMULTI -TASK TRAINING 89.51 49.58 63.29 65.50\\nsequence [ˆri,ˆyi]and treat the entire sequence as\\nthe target output in training small models, as con-\\nsidered in (Magister et al., 2022; Ho et al., 2022):\\nLsingle =1\\nNNX\\ni=1ℓ(f(xi),[ˆri,ˆyi]). (5)\\nIn Table 2, we compare this single-task training\\napproach to our proposed multi-task training ap-\\nproach for utilizing LLM-rationales. We see that\\nnot only multi-task training consistently leads to\\nbetter performance, single-task training with LLM-\\nrationales can at times leads to worse performance\\nthan standard finetuning, e.g., on ANLI and CQA.\\nIn fact, similar results have also been observed\\nin (Wiegreffe et al., 2021; Magister et al., 2022;\\nHo et al., 2022) that simply treating rationale and\\nlabel predictions as a single joint task may harm the\\nmodel’s performance on label prediction. This val-\\nidates our use of the multi-task training approach,\\nand highlights the need to treat the rationales care-\\nfully so as to unleash their actual benefits.\\n5 Discussion\\nWe propose Distilling step-by-step to extract ra-\\ntionales from LLMs as informative supervision in\\ntraining small task-specific models. We show that\\nDistilling step-by-step reduces the training dataset\\nrequired to curate task-specific smaller models; it\\nalso reduces the model size required to achieve,\\nand even surpass, the original LLM’s performance.\\nDistilling step-by-step proposes a resource-efficient\\ntraining-to-deployment paradigm compared to ex-\\nisting methods. Further studies demonstrate the\\ngeneralizability and the design choices made in\\nDistilling step-by-step. Finally, we discuss the lim-\\nitations, future directions and ethics statement of\\nour work below.', metadata={'source': './docs/distilling-LLM.pdf', 'page': 8}),\n",
              " Document(page_content='Limitations\\nThere are a number of limitations with our ap-\\nproach. First, we require users to produce a few\\nexample demonstrations ( ∼10-shot for all tasks)\\nin order to use the few-shot CoT (Wei et al., 2022)\\nprompting mechanism. This limitation can be\\nimproved by using recent advances that suggest\\nthat rationales can be elicited without any user-\\nannotated demonstrations (Kojima et al., 2022).\\nSecond, training task-specific models with ratio-\\nnales incur slight training-time computation over-\\nhead. However, at test time, our multi-task design\\nnaturally avoids the computation overhead since it\\nallows one to only predict labels without generat-\\ning the rationales. Finally, while we observe suc-\\ncess using LLM rationales, there is evidence that\\nLLMs exhibit limited reasoning capability on more\\ncomplex reasoning and planning tasks (Valmeekam\\net al., 2022). Future work should characterize how\\nrationale quality affects Distilling step-by-step.\\nEthics statement\\nIt is worth noting that the behavior of the our down-\\nstream smaller models is subject to biases inherited\\nfrom the larger teacher LLM. We envision that the\\nsame research progress in reducing anti-social be-\\nhaviors in LLMs can also be applied to improve\\nsmaller language models.\\nReferences\\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\\nMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\\nDipanjan Das, and Mirella Lapata. 2022. Qameleon:\\nMultilingual qa with only 5 examples. arXiv preprint\\narXiv:2211.08264 .\\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-\\nrel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-\\neric Sala, and Christopher Ré. 2022. Ask me any-\\nthing: A simple strategy for prompting language mod-\\nels.arXiv preprint arXiv:2210.02441 .\\nLucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Mar-\\nkeeva, Rohan Anil, and Alexander Kolesnikov. 2022.\\nKnowledge distillation: A good teacher is patient and\\nconsistent. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition ,\\npages 10925–10934.\\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\\nthony, Leo Gao, Laurence Golding, Horace He, Con-\\nnor Leahy, Kyle McDonell, Jason Phang, Michael\\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\\nLaria Reynolds, Jonathan Tow, Ben Wang, andSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\\nsource autoregressive language model. In Proceed-\\nings of the ACL Workshop on Challenges & Perspec-\\ntives in Creating Large Language Models .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems , 33:1877–1901.\\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\\nNiculescu-Mizil. 2006. Model compression. In Pro-\\nceedings of the 12th ACM SIGKDD international\\nconference on Knowledge discovery and data mining ,\\npages 535–541.\\nOana-Maria Camburu, Tim Rocktäschel, Thomas\\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\\nral language inference with natural language expla-\\nnations. Advances in Neural Information Processing\\nSystems , 31.\\nTing Chen, Simon Kornblith, Kevin Swersky, Moham-\\nmad Norouzi, and Geoffrey E Hinton. 2020. Big\\nself-supervised models are strong semi-supervised\\nlearners. Advances in neural information processing\\nsystems , 33:22243–22255.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. Bert: Pre-training of deep\\nbidirectional transformers for language understand-\\ning. arXiv preprint arXiv:1810.04805 .\\nJacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael\\nCollins, and David Mimno. 2022. Honest students\\nfrom untrusted teachers: Learning an interpretable\\nquestion-answering pipeline from a pretrained lan-\\nguage model. arXiv preprint arXiv:2210.02498 .\\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\\nTushar Khot. 2023. Specializing smaller language\\nmodels towards multi-step reasoning. arXiv preprint\\narXiv:2301.12726 .\\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\\nMazare, and Jason Weston. 2019. Learning from\\ndialogue after deployment: Feed yourself, chatbot!\\narXiv preprint arXiv:1901.05415 .\\nPeter Hase and Mohit Bansal. 2021. When can models\\nlearn from explanations? a formal framework for\\nunderstanding the roles of explanation data. arXiv\\npreprint arXiv:2102.02201 .\\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\\nDistilling the knowledge in a neural network. arXiv\\npreprint arXiv:1503.02531 , 2(7).', metadata={'source': './docs/distilling-LLM.pdf', 'page': 9}),\n",
              " Document(page_content='Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\\nLarge language models are reasoning teachers. arXiv\\npreprint arXiv:2212.10071 .\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\\nford, Diego de Las Casas, Lisa Anne Hendricks,\\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\\ning compute-optimal large language models. arXiv\\npreprint arXiv:2203.15556 .\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model fine-tuning for text classification.\\nInProceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1:\\nLong Papers) , pages 328–339, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\\nLarge language models can self-improve. arXiv\\npreprint arXiv:2210.11610 .\\nFotis Iliopoulos, Vasilis Kontonis, Cenk Baykal, Gau-\\nrav Menghani, Khoa Trinh, and Erik Vee. 2022.\\nWeighted distillation with unlabeled examples. In\\nAdvances in Neural Information Processing Systems .\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. arXiv preprint\\narXiv:2205.11916 .\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\\nThe power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691 .\\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xi-\\nang Ren, Kai-Wei Chang, and Yejin Choi. 2023.\\nSymbolic chain-of-thought distillation: Small mod-\\nels can also\" think\" step-by-step. arXiv preprint\\narXiv:2306.14050 .\\nKevin J Liang, Weituo Hao, Dinghan Shen, Yufan\\nZhou, Weizhu Chen, Changyou Chen, and Lawrence\\nCarin. 2020. Mixkd: Towards efficient distilla-\\ntion of large-scale language models. arXiv preprint\\narXiv:2011.00593 .\\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\\nTeaching small language models to reason. arXiv\\npreprint arXiv:2212.08410 .\\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\\n2020. A diverse corpus for evaluating and developing\\nenglish math word problem solvers. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 975–984.\\nSmitha Milli, Ludwig Schmidt, Anca D Dragan, and\\nMoritz Hardt. 2019. Model reconstruction from\\nmodel explanations. In Proceedings of the Confer-\\nence on Fairness, Accountability, and Transparency ,\\npages 1–9.Sharan Narang, Colin Raffel, Katherine Lee, Adam\\nRoberts, Noah Fiedel, and Karishma Malkan. 2020.\\nWt5?! training text-to-text models to explain their\\npredictions. arXiv preprint arXiv:2004.14546 .\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\\nJason Weston, and Douwe Kiela. 2020. Adversarial\\nNLI: A new benchmark for natural language under-\\nstanding. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics . As-\\nsociation for Computational Linguistics.\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\\nHenryk Michalewski, Jacob Austin, David Bieber,\\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\\nDavid Luan, et al. 2021. Show your work: Scratch-\\npads for intermediate computation with language\\nmodels. arXiv preprint arXiv:2112.00114 .\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\\n2021. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 2080–2094, Online.\\nAssociation for Computational Linguistics.\\nDanish Pruthi, Rachit Bansal, Bhuwan Dhingra,\\nLivio Baldini Soares, Michael Collins, Zachary C\\nLipton, Graham Neubig, and William W Cohen.\\n2022. Evaluating explanations: How much do ex-\\nplanations from the teacher aid students? Transac-\\ntions of the Association for Computational Linguis-\\ntics, 10:359–375.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research ,\\n21(140):1–67.\\nNazneen Fatema Rajani, Bryan McCann, Caiming\\nXiong, and Richard Socher. 2019. Explain your-\\nself! leveraging language models for commonsense\\nreasoning. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 4932–4942, Florence, Italy. Association for\\nComputational Linguistics.\\nAndrew Slavin Ross, Michael C Hughes, and Finale\\nDoshi-Velez. 2017. Right for the right reasons: Train-\\ning differentiable models by constraining their expla-\\nnations. arXiv preprint arXiv:1703.03717 .\\nRyan Smith, Jason A Fries, Braden Hancock, and\\nStephen H Bach. 2022a. Language models in the\\nloop: Incorporating prompting into weak supervision.\\narXiv preprint arXiv:2205.02318 .\\nShaden Smith, Mostofa Patwary, Brandon Norick,\\nPatrick LeGresley, Samyam Rajbhandari, Jared\\nCasper, Zhun Liu, Shrimai Prabhumoye, George\\nZerveas, Vijay Korthikanti, et al. 2022b. Using\\ndeepspeed and megatron to train megatron-turing nlg', metadata={'source': './docs/distilling-LLM.pdf', 'page': 10}),\n",
              " Document(page_content='530b, a large-scale generative language model. arXiv\\npreprint arXiv:2201.11990 .\\nSuraj Srinivas and François Fleuret. 2018. Knowledge\\ntransfer with jacobian matching. In International\\nConference on Machine Learning , pages 4723–4731.\\nPMLR.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. CommonsenseQA: A ques-\\ntion answering challenge targeting commonsense\\nknowledge. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , pages\\n4149–4158, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\\nVechtomova, and Jimmy Lin. 2019. Distilling task-\\nspecific knowledge from bert into simple neural net-\\nworks. arXiv preprint arXiv:1903.12136 .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applica-\\ntions. arXiv preprint arXiv:2201.08239 .\\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\\nand Subbarao Kambhampati. 2022. Large language\\nmodels still can’t plan (a benchmark for llms on plan-\\nning and reasoning about change). arXiv preprint\\narXiv:2206.10498 .\\nPeifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,\\nand Xiang Ren. 2022a. Pinto: Faithful language\\nreasoning using prompt-generated rationales. arXiv\\npreprint arXiv:2211.01562 .\\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\\nZhu, and Michael Zeng. 2021. Want to reduce\\nlabeling cost? gpt-3 can help. arXiv preprint\\narXiv:2108.13487 .\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022b. Self-consistency\\nimproves chain of thought reasoning in language\\nmodels. arXiv preprint arXiv:2203.11171 .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. arXiv preprint arXiv:2201.11903 .\\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\\nHwang, Liwei Jiang, Ronan Le Bras, Ximing\\nLu, Sean Welleck, and Yejin Choi. 2021. Sym-\\nbolic knowledge distillation: from general language\\nmodels to commonsense models. arXiv preprint\\narXiv:2110.07178 .\\nSarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith.\\n2021. Measuring association between labels and\\nfree-text rationales. In Proceedings of the 2021 Con-\\nference on Empirical Methods in Natural LanguageProcessing , pages 10266–10284, Online and Punta\\nCana, Dominican Republic. Association for Compu-\\ntational Linguistics.\\nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007.\\nUsing “annotator rationales” to improve machine\\nlearning for text categorization. In Human Language\\nTechnologies 2007: The Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics; Proceedings of the Main Confer-\\nence, pages 260–267, Rochester, New York. Associa-\\ntion for Computational Linguistics.\\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\\n2022. Star: Bootstrapping reasoning with reason-\\ning. arXiv preprint arXiv:2203.14465 .\\nJeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas\\nGuibas, and Jitendra Malik. 2020. Side-tuning: a\\nbaseline for network adaptation via additive side net-\\nworks. In European Conference on Computer Vision ,\\npages 698–714. Springer.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\\nOpt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068 .\\nYe Zhang, Iain Marshall, and Byron C. Wallace. 2016.\\nRationale-augmented convolutional neural networks\\nfor text classification. In Proceedings of the 2016\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 795–804, Austin, Texas.\\nAssociation for Computational Linguistics.\\nLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao\\nZhuang, Zhifeng Chen, Yanping Huang, Yida Wang,\\nYuanzhong Xu, Danyang Zhuo, Joseph E Gonza-\\nlez, et al. 2022. Alpa: Automating inter-and intra-\\noperator parallelism for distributed deep learning.\\narXiv preprint arXiv:2201.12023 .', metadata={'source': './docs/distilling-LLM.pdf', 'page': 11}),\n",
              " Document(page_content='A Experiment detail\\nA.1 Implementation\\nWe perform our experiments on cloud A100 ×16\\nGPU instances. We train the T5 models with\\nthe following hyperparameters, using publicly\\navailable packages from https://github.com/\\nhuggingface/transformers :\\n•T5-Base ( 220M) and T5-Large ( 770M): We\\ntrain the models with learning rate = 5×\\n10−5,batch size = 64 ,max input length =\\n1024 , for a maximum of 10000 steps.\\n•T5-XXL ( 11B): We train the models with\\nlearning rate = 5×10−5,batch size = 32 ,\\nmax input length = 1024 , for a maximum of\\n4000 steps.\\nWe report all the results over 4random runs, and\\ninclude the standard error in the presented plots.\\nA.2 Datasets\\nWe provide more detailed descriptions on the\\ndatasets used in our experiments. We include the\\nsources from which we obtain the datasets as well\\nas their original sources released from the authors.\\nWe refer readers to these sources for their license or\\nterms for use and/or distribution. To the best of our\\nknowledge, the datasets used do not contain infor-\\nmation that names or uniquely identifies individual\\npeople or offensive content.\\n•e-SNLI: The dataset was originally re-\\nleased in (Camburu et al., 2018), and made\\npublicly available at https://github.com/\\nOanaMariaCamburu/e-SNLI . We obtain\\nthe dataset from https://huggingface.co/\\ndatasets/esnli .\\n•ANLI: The dataset was originally released\\nin (Nie et al., 2020), and made pub-\\nlicly available at https://github.com/\\nfacebookresearch/anli . We obtain the\\ndataset from https://huggingface.co/\\ndatasets/anli . We use the R1 split in our\\nexperiments.\\n•CQA: The dataset was originally released\\nin (Talmor et al., 2019), and made publicly\\navailable at https://www.tau-nlp.sites.\\ntau.ac.il/commonsenseqa . It was then\\naugmented with human-labeled explanationsTable 3: Dataset statistics used in our experiments.\\nDataset Train Validation Test\\ne-SNLI 549,367 9,842 9,824\\nANLI 16,946 1,000 1,000\\nCQA 8,766 975 1,221\\nSV AMP 720 80 200\\nby (Rajani et al., 2019), which is avail-\\nable at https://github.com/salesforce/\\ncos-e . We obtain the dataset used in our ex-\\nperiments from https://huggingface.co/\\ndatasets/cos_e .\\n•SV AMP: The dataset was originally re-\\nleased in (Patel et al., 2021). We ob-\\ntain the dataset from https://github.com/\\narkilpatel/SVAMP .\\n•ASDiv: The dataset was originally re-\\nleased in (Miao et al., 2020). We ob-\\ntain the dataset from https://github.com/\\nchaochun/nlu-asdiv-dataset .\\nFor each dataset, we randomly subsample 10%\\nof the original training set to serve as validation set\\nwhen validation set is not originally provided. For\\nCQA, we use the original validation set to serve\\nas our test set since the ground-truth labels are not\\navailable for the original test set. We provide the\\ndataset statistics in Table 3.', metadata={'source': './docs/distilling-LLM.pdf', 'page': 12})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc9grhjvrJ1O",
        "outputId": "ee0e33bb-42ab-4b13-87cc-710b34af62e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Split the Document into Chunks**"
      ],
      "metadata": {
        "id": "uITod3JGL59W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "CkOAlCQaK7xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks=document_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "Rx-Kg9NEMF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(document_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SbVQuZ0MRTd",
        "outputId": "3532468b-f012-4a2b-f993-e3863c521d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knyzL2cDMNnF",
        "outputId": "abc5b84e-38de-4b26-df40-5aacdce71fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='SKILL: Structured Knowledge Infusion for Large Language Models\\nFedor Moiseev∗1,2Zhe Dong†2Enrique Alfonseca2Martin Jaggi1\\n1EPFL, Switzerland2Google, Switzerland\\n{femoiseev, zhedong, ealfonseca}@google.com, martin.jaggi@epfl.ch\\nAbstract\\nLarge language models (LLMs) have demon-\\nstrated human-level performance on a vast\\nspectrum of natural language tasks. However,\\nit is largely unexplored whether they can bet-\\nter internalize knowledge from a structured\\ndata, such as a knowledge graph, or from text.\\nIn this work, we propose a method to infuse\\nstructured knowledge into LLMs, by directly\\ntraining T5 models on factual triples of knowl-\\nedge graphs (KGs). We show that models pre-\\ntrained on Wikidata KG with our method out-\\nperform the T5 baselines on FreebaseQA and\\nWikiHop, as well as the Wikidata-answerable\\nsubset of TriviaQA and NaturalQuestions. The\\nmodels pre-trained on factual triples compare\\ncompetitively with the ones on natural lan-\\nguage sentences that contain the same knowl-\\nedge. Trained on a smaller size KG, Wiki-\\nMovies, we saw 3×improvement of exact\\nmatch score on MetaQA task compared to T5\\nbaseline. The proposed method has an advan-\\ntage that no alignment between the knowledge\\ngraph and text corpus is required in curating\\ntraining data. This makes our method particu-\\nlarly useful when working with industry-scale\\nknowledge graphs.\\n1 Introduction\\nLarge pre-trained language models, such as BERT\\n(Devlin et al., 2019), GPT-3 (Brown et al., 2020),\\nT5 (Raffel et al., 2019), REALM (Guu et al., 2020)\\nand ERNIE (Sun et al., 2021) have become the\\nstate-of-the-art technology for many tasks. They\\nare commonly pre-trained using unstructured text\\ncorpora, on tasks such as next word prediction,\\nnext sentence prediction (NSP) or masked lan-\\nguage modelling (MLM). Especially for T5, self-\\nsupervised learning on unlabelled text corpus with\\nMLM has been a common pre-training recipe\\n(Roberts et al., 2020). This is normally followed\\n∗Work done during internship at Google.\\n†Correspondence Author.by a ﬁne-tuning step on the task of interest (Ruder\\net al., 2019), although large language models have\\nalso proved useful without this task-speciﬁc ﬁne-\\ntuning (Brown et al., 2020).\\nBeyond the capacity of contextual understand-\\ning, human-level language understanding pivots on\\nthe knowledge about the world. The world knowl-\\nedge is often expressed as factual triples (c.f. Ji\\net al., 2020), in the form of ( subject entity ,relation ,\\nobject entity ). A knowledge graph (KG) deﬁned by\\na set of factual triples consists of the subjects and\\nobjects as vertices/nodes, and the relations form-\\ning the edges connecting them. Most of the large\\nscale KGs (e.g. Wikidata, Vrande ˇci´c and Krötzsch,\\n2014) are stored in triple format.\\nLLMs demonstrate some capacity of learning\\nworld knowledge from the natural text corpus\\n(Roberts et al., 2020), but it is unclear to what\\ndegree they are also able to learn and memorize\\nnew knowledge directly from structured KG triples,\\nor from text describing them explicitly.\\nIn order to infuse knowledge into a LLM, one\\noption is to generate a textual version of the knowl-\\nedge base, and apply the standard training objec-\\ntives, e.g. MLM. This is unfortunately highly non-\\ntrivial. One can either align sentences with KG\\ntriples, as done in ERNIE (Sun et al., 2021), or\\ngenerate sentences from triples, as done in KELM\\n(Agarwal et al., 2021). These approaches are un-\\nfortunately hard to port to knowledge graphs with\\ndifferent schemas. These processes are also lossy\\nin that not every triple can be aligned or produce\\na valid sentence, and there is not a good under-\\nstanding whether this can introduce unnecessary\\nselection biases on top of biases existing in the\\noriginal KG.\\nIn this work, we propose a method of Knowl-\\nedge Infusion for Large Language Models\\n(SKILL) , where LLMs directly learns from knowl-\\nedge triples. Experiment results shows the check-\\npoints trained with proposed method on WikidataarXiv:2205.08184v1  [cs.CL]  17 May 2022', metadata={'source': './docs/KG-LLM.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p02zc4-KfQZf",
        "outputId": "c829d601-08a9-46dc-c1e6-30f6f811ffbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='KG outperform the T5 baselines on four standard\\nclosed-book question-answering (QA) tasks. With\\na smaller KG, WikiMovies, the proposed method\\ngain3×exact match score performance improve-\\nment on MetaQA task. The models learning di-\\nrectly from knowledge triples performs competi-\\ntively with the ones with the aligned natural sen-\\ntences that contain the same amount of knowledge.\\nBeing able to learn directly from knowledge triples\\nenables easy addition of structured knowledge into\\nlanguage modeling pre-training.\\n2 Related work\\nPrevious works that use knowledge graphs to en-\\nhance the quality of knowledge-intensive down-\\nstream tasks can be divided into two groups: using\\nknowledge graphs at the inference time, and in-\\nfusing knowledge into the model weights at the\\npre-training time. The proposed method falls in the\\nlatter group.\\nExplicit usage of knowledge graphs. A\\nretrieval-augmented model is commonly used,\\nin order to retrieve and apply the knowledge\\nfrom external memories or sources. FILM (Verga\\net al., 2021) and EaE (Févry et al., 2020) extend\\nTransformer (Vaswani et al., 2017) models with\\nexternal entity (both FILM and EaE) and fact\\n(FILM) memories. REALM (Guu et al., 2020)\\nis pre-trained to perform reasoning over a large\\ntextual knowledge corpus on-the-ﬂy during infer-\\nence. UniK-QA (Oguz et al., 2020) combines the\\nstructured and unstructured information to improve\\nthe open-domain QA tasks with a retriever-reader\\nframework. The main difference between the\\nproposed method, SKILL, and retrieval-augmented\\nmodels is that SKILL doesn’t introduce retrieval\\nsystem or external memories to the model, but\\nit directly embeds knowledge into the model\\nparameters, which introduces no extra cost at\\ninference time.\\nKnowledge infusion. A common way of param-\\neterized knowledge infusion is to map or convert\\nstructured knowledges into natural language text.\\nERNIE 3.0 (Sun et al., 2021) trains a knowledge-\\nenhanced model on a corpus combining triples and\\ntheir aligned sentences, by randomly masking re-\\nlation in a triple or words in a sentence. On the\\ncontrary, SKILL trains only on triples.\\nKnowBert (Peters et al., 2019) incorporates\\nknowledge from Wikipedia and WordNet (Miller,1995) into a BERT model through entity\\nembeddings with knowledge-attention and re-\\ncontextualization mechanism. BERT-MK (He et al.,\\n2020) is a BERT-based model that integrates graph\\ncontextual knowledge of a medical KG, which\\ndemonstrates the utility of graph-level knowledge.\\nThese approaches requires entity linking and sen-\\ntences contextualizing the knowledge graph infor-\\nmation.\\nKG-FiD (Yu et al., 2021) extends the Fusion-in-\\nDecoder model (Izacard and Grave, 2021), with a\\nmodule that ﬁlters and re-ranks passages based on\\nstructural connections in knowledge graph between\\nentities described in those passages. In contrast to\\nthe SKILL method that we propose, it requires\\nthe existence of natural text passages describing\\neach knowledge graph entity, so Wikipedia corpus\\nwas used since it naturally provides articles that\\ndescribe entities.\\nHeinzerling and Inui (2021) explored the ability\\nof language models to memorize and understand\\ninformation from knowledge graphs, but used nat-\\nural language representation of triples based on\\npredeﬁned templates instead of structured represen-\\ntation. Usage of predeﬁned templates signiﬁcantly\\nlimits scalability and therefore only relatively small\\nknowledge graphs were used, such as Google-RE1.\\nIn contrast to the new method presented in this\\npaper, all of these approaches require an explicit\\nmapping between the knowledge graph entities\\nor facts and corresponding natural language sen-\\ntences, which can limit applications to industry-\\nscale knowledge graphs that don’t have such a map-\\nping.\\nDifferent goals of using knowledge graphs.\\nBesides that, some papers embed knowledge into\\nmodel weights but pursue different goals rather\\nthan improving performance on downstream tasks.\\nCOMET (Bosselut et al., 2019) is most similar to\\nour work and trains a commonsense-aware Trans-\\nformer Language Model by learning to generate\\nloosely structured commonsense descriptions in the\\nnatural language given the structured knowledge.\\nSimilar to us, it also uses KG triples in surface\\nform as a source for training data, but in contrast\\nto our research, the ﬁnal goal of COMET is to gen-\\nerate new knowledge instead of utilizing existing\\nones. Another important difference is the scale:\\nCOMET uses Atomic (Sap et al., 2019) and Con-\\n1https://ai.googleblog.com/2013/04/50000-lessons-on-\\nhow-to-read-relation.html', metadata={'source': './docs/KG-LLM.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5: Download the Embeddings from Hugging Face, Download the Sentence Transformer Embeddings**"
      ],
      "metadata": {
        "id": "A3DdRaiAMeK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "A138fmmJMQtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"]=\"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\""
      ],
      "metadata": {
        "id": "1P9Dbm_cxtq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "vZCkHmS_xszP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mf6GzRYMzWt",
        "outputId": "fc0395e9-39ca-4299-e354-d9efaa8e7011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Setting Up Chroma as our Vector Database**"
      ],
      "metadata": {
        "id": "mjmvoY9GNTTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the Document Chunks into Embedding and save them to the vector store"
      ],
      "metadata": {
        "id": "VJ6jc4vCSS6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')"
      ],
      "metadata": {
        "id": "VoWH6s_xNQYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "csmhafp41feh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 7: Login into Hugging Face Account to Download the Model**"
      ],
      "metadata": {
        "id": "iKK7UgGLOkcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "7c97f4dfbfe34958adc6618c3b24b19f",
            "9ad38378ad7a450ebf09a986909d5305",
            "d499a2a70c834dada215f0105355150c",
            "f01b1fc27a984827acb30ebf02cb0fa1",
            "fee02776802e46d5a545d3845393585f",
            "00f06e5b6b74407a88767375884fbf63",
            "b52bcf840e4445a191f2e8ee0b5789d7",
            "79702cb4dd5b400e98073b2e3a64556e",
            "aa19ed252eda40ccbebb1c268620013a",
            "97794bf16f0141f39c63098c79945e84",
            "3b4fc0778cf7436386907d177d718be8",
            "63f4a9320cb5460c9141cf6f7f6c1279",
            "7e4226b5361f43bcb803a4bdd42e3930",
            "d731868eec474bd5a42edff18c990085",
            "a8f7a943c65e499ba2312a1ede7e3968",
            "445381174d9841e6933d5f24cb686863",
            "42cae858a1ac4983a25b76aef86a4960"
          ]
        },
        "id": "qg9Zn9vqOjt2",
        "outputId": "e55f7c29-424c-418b-bb46-330785e25f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c97f4dfbfe34958adc6618c3b24b19f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 8: Download the Llama 2 7B Chat Model**"
      ],
      "metadata": {
        "id": "1Mqr0HMBOz6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "X4kpkAKJchw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_auth = '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'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"abhishek/llama-2-7b-hf-small-shards\",\n",
        "                                          use_auth_token=hf_auth)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"abhishek/llama-2-7b-hf-small-shards\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=hf_auth,\n",
        "                                              #load_in_8bit=True,\n",
        "                                              load_in_4bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "df4eb3e94e1141f3b5c6e5d3e17cfa89",
            "956b9bd4bb754048bae343eea6c03e6d",
            "995fe5f6257645c89ad8af4f0666c47c",
            "bddfea4a4c934c4bb6120f849fc3edb7",
            "0b721e2b559a42e0b12f13b1a539ba14",
            "7609836970764855bf1f4bc23c16e592",
            "84b2f030cfd34371b4d99cd66da68a26",
            "8c7abd404a864b138a986920d255218a",
            "766c4a1a20f8473da9d7d74712553ec6",
            "099c61f6b83945a2afa37bbce4f7e699",
            "2371b48b9ca14b1fad6a24518a1bd9ae",
            "d915fa5fb1d5432981326c5c43c8e0ed",
            "549aff90cb1c42dc96bae0dd32e32bfc",
            "c4286162d8dc4f829f322607958f1c42",
            "6f6e720fa9e84e38a22b3b374b0f3936",
            "f10bf43e45dd42a094882ab45ed118c4",
            "30cfc81d7f92481bbbcbc2ca66deb9da",
            "c15ee309f09e419eae514221e083672a",
            "b82acde8382f4cbd8061c22066c6f3ad",
            "763d7517ff31481390076fbd1bb5d791",
            "6df9d3674ccc459799f57303815a91f2",
            "78cca83ded014b9383b98f7ac5d04076",
            "fa7077e06a3e4880b23a17f16684855e",
            "0174e9980b1a49ea86aea6884a712bae",
            "7c5054403e0b4f51b9c34b8db50434d3",
            "9766ec66fcc7457aa0f90bf5c965a87f",
            "eb755ecaa63f42d2a5e2376bbb3282fc",
            "3e06e8fb16cb4e2cbec03189992bf800",
            "4fde35aef01547878bd91ff9fee11faf",
            "d02ae7365a284d1fbe8ab4e4db190943",
            "d99c9c60bc064898980722a23471dc36",
            "98dd63d181734b75a89ad1006673f4e2",
            "39bd5ae287254be3966c88e37a895b61",
            "0911e61804ec42c89876a686f6920b77",
            "94e10736a7924b1190d303408ca1151a",
            "0e5bacf1ae524b87bb1b867d5ac0bd84",
            "4e9fbd8b824246778d6a8090d99a743e",
            "8b5dcd5e73144a1099c47708a8db195d",
            "0f579070eab0426a98b58d2b04525b77",
            "74049dfd556e4358a7baaa5621b419d3",
            "89392e4ac28d4a3f90d9a1227e505de1",
            "a40f29c89a164710b90a54e18e1b76ce",
            "601789241e4f4f4b989dafadfc7dde19",
            "190e66177b384643ac35ff794e3fb0e5",
            "4701ef625a4e49578277c8e7eadefdcf",
            "dcb76771651b4b8684ad335e8bfeff03",
            "e62bbee0388e4699a30c2b30d802d979",
            "efea2c0998f6430b98645324e550b691",
            "09bfab36b5884908a83f723ee62e751d",
            "f04da22f1ab04cc498da975c45a0266e",
            "c797a4b5421c4a03b616c8e32bf0051f",
            "08fb543e3c3a40a08e07461a11a57605",
            "7fc5a27cef8844019c9b2f5d7e2690a0",
            "7b0660b61df14c18a531d80a701e757a",
            "2465971ce5f4453db3156ad9314ddf5d",
            "52844abbdd5248aebc8bb60df8be8dce",
            "0f505a3a0dd34920ac3cffcd70fda47d",
            "21fbb393a23f49ceb4466f6c894a501e",
            "9dc77e6771d644968d4e5a9288f9b299",
            "177b2a6bc7ac4b5d92bbb39aa62c9d8e",
            "fb215d566c664202925e579ac9e83505",
            "7cb6f83a234d434ebfef1beb665f4c35",
            "79857fd6011543ffb71e6bafc671685e",
            "d709dc86ee6b43cbb8b55a751f487ecb",
            "922a93c8d09b4c4b90059b42d603a15d",
            "2733b4eb89cc49e487bf7f5c2fe897f1",
            "7a6ed97cfa194591a740ef5220fbd25b",
            "cfb5bf2bca4a40cdba3817c4f120d158",
            "09496b3c7c5a482586b2ee2b929ab47c",
            "937e27badb7a43e0ac9505e47ac21787",
            "c39b3a80b85c4a09b9513fce102790de",
            "11a2069c74904b63bafd87005342162e",
            "b8bf9d45ed1e40b98a36b2fa0f6a7e1f",
            "655a2b41c9084b69acaa243d64201b37",
            "6e967816905d478581c77847b51a8c22",
            "d46d5ac0ee6c49f9982ecda46504b1b6",
            "e6444926b2ad4a04a3ab1cad0a6c6f7c",
            "575f60da6b4c46258bd61cf9bbda2196",
            "3ebf092f7abb49e88883ca78dde3ce4c",
            "dd7a36d8333548f596642d1d62eca92c",
            "4525c0d67d0e48e485325962b9a3fffb",
            "3678eca8a0ee4734bfa31778bca93098",
            "0feb27361d914c75950d9eb8dab43c82",
            "0deaabc1a5ec42d1a3b6cb068a145cf3",
            "4d146616507a447580fbe5350dcee2da",
            "d5659ff3b31946328d65f089f4aabb4f",
            "c3d98b27d5cd4e80a1be934ebb74d54a",
            "037d7d6cded44ad6bdd79e8be753da30",
            "59b292f3c1cd4766b0172e65f16b35d3",
            "be7dfd3115034b858111b7093174fe52",
            "67c35692011249f0b2974e87027ea49d",
            "8832aa29b4104cd49b31a86eb8036408",
            "0da06dc077c046db859724af55b7b560",
            "c9eb259f9dff466dba6ba7aad1aeec30",
            "de87b9dada0340fd9aab6a9a69215c19",
            "399a1a4bddce4359b4a49b69c943da2d",
            "705ce7fe745b467aac39e11cd805ef00",
            "db7415630f664fc7a93e16af575f97a0",
            "02895c9f2d0e4be0928d624b5314e1a9",
            "b0083d7c30a94c13863e1b02a14fcf14",
            "58b6d9bd13bd462c96211eefb2c060aa",
            "29fa906cc9324060a99418f15d773fff",
            "5b69dd5ad1884c30b3b40586b376b1dd",
            "71300263b6fe4307b0f744bec7fd6a3b",
            "453adbb6182240f588358cf1ea3fb417",
            "06b721de73b3467fb285aee7696d4d6e",
            "c85815434c874134b9262159771a2c18",
            "5048654f32ef4f0a81af1ac25b15b380",
            "e9d9b44d937d4064a7f610bf0465e89b",
            "7501e83aeb3c4cc6bb891a88a757ddc6",
            "c2a3887452984984ae7773cf275405dc",
            "21310fdd25e84c09aac4590cb43a0434",
            "47df6a814f0d4f61988a438698bdbbcc",
            "46c436c148ed42738f978a6721259e5f",
            "0fac26f4b55b4845a6a28ba5827dbadc",
            "91e56eb71b8e4dd397f37014ce265f09",
            "de72dfc7a68448079bf9cb96e9a0852d",
            "8426efa0f9f449cfbd0b354d2fb7ec62",
            "15ec9f3cbbb3443a89a7acf1b52459fc",
            "04d1cbb39a1c4abaae5b4b2ab1e5e18e",
            "91447c43539f4407ad2838b9a12a8150",
            "65e9055aa9b2460c864076bfdc865868",
            "078f8c29563a4ad08d676237a0cf6298",
            "071c7761c13d4f71b47c4efc4b670813",
            "632b42a9dd2549169544a649b6962387",
            "05e2a3f876cf4432a0917f867a09eed3",
            "f9acb4c4e4124b9bb76c1b5519d03461",
            "b1cd73f50d984c02949e8f6bd71a6713",
            "5c0cdd47363c4d758afc0ca48bd61524",
            "827378ba4f8e47ac982a5c211f632a82",
            "1676f9e03db74556bc64479c42893b2c",
            "e31a5b0f25e245f69f1371c704d7c6e0",
            "7b580f04efa949728ebb0960faae7d36",
            "6bca494820d64308b1715417f19b1b9d",
            "e91c96a264f84b32a1fa21356e1fb657",
            "dd16798f7fcd4a478a824d0cfb26e47c",
            "3883d62462ad48eea31c9fdf11cee093",
            "769085ae00b640a0a4724729d140b3e6",
            "620ec9dcf65144c5acff053f3e02c543",
            "e7dbc5eea5f84f3182bc0564002d808a",
            "172761beaedf4d898ac9efc1f2200e2c",
            "6d4d8d6276714bc78583ab9001f0b4b2",
            "0c134b56af8942558afa200f8bd10587",
            "e6a32f24aee940199a729906503a001a",
            "5b5b356d6ef742deb3aa9efa204de3e4",
            "ee3e2c657a2145fa870745caeac7e535",
            "2d554f3a999c45959638790ec61b48e4",
            "4a863bff7b474e3c8057a494a1f9094a",
            "3ba78d5eb73f4c6fbe63f133b5d9d076",
            "244613ffc5dd4cc2b2cd1977f17e7777",
            "465c5f52b5f24846b7f402ed5f80693c",
            "f32cbb59daf740eebbe8077d4045d425",
            "03183ae108c046638b7f0ce7189eae77",
            "b8e1a9e9f3904da58b0dc20854384aaf",
            "4d0cd744c0b645feb4e4a1c5976c5fda",
            "407f477ce15c421eb9a30bc2ef2cc026",
            "613635c363594cfa8cf47ad03fc2edbb",
            "9a6922f4b06d4d5aa6e93ec70c476478",
            "90c9854e7e8244419591e8c5f3cdf499",
            "a4eac48bfed54ad4bbec93a03a61928f",
            "4451182adaa24c67b4783038e7be414f",
            "cb77ad3db2974df990bf32ae5e6587a7",
            "98ebb1f377ae42179437216ae07581b5",
            "675e964cf4fc4a7997d43642641bff4a",
            "e6517290a69d494ab11fb39b3bed8a97"
          ]
        },
        "id": "toVnNgn8VsDZ",
        "outputId": "17916b6f-8a03-4fbe-dcfe-7106019f5358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df4eb3e94e1141f3b5c6e5d3e17cfa89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d915fa5fb1d5432981326c5c43c8e0ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa7077e06a3e4880b23a17f16684855e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00001-of-00010.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0911e61804ec42c89876a686f6920b77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00002-of-00010.bin:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4701ef625a4e49578277c8e7eadefdcf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00003-of-00010.bin:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52844abbdd5248aebc8bb60df8be8dce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00004-of-00010.bin:   0%|          | 0.00/2.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a6ed97cfa194591a740ef5220fbd25b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00005-of-00010.bin:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "575f60da6b4c46258bd61cf9bbda2196"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00006-of-00010.bin:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b292f3c1cd4766b0172e65f16b35d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00007-of-00010.bin:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0083d7c30a94c13863e1b02a14fcf14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00008-of-00010.bin:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2a3887452984984ae7773cf275405dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00009-of-00010.bin:   0%|          | 0.00/2.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65e9055aa9b2460c864076bfdc865868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00010-of-00010.bin:   0%|          | 0.00/705M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b580f04efa949728ebb0960faae7d36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6a32f24aee940199a729906503a001a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d0cd744c0b645feb4e4a1c5976c5fda"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 9: Creating a Hugging Face Pipeline**"
      ],
      "metadata": {
        "id": "_CH-j3rzQ7j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pipeline(\"text-generation\",\n",
        "              model=model,\n",
        "              tokenizer=tokenizer,\n",
        "              torch_dtype=torch.bfloat16,\n",
        "              device_map='auto',\n",
        "              max_new_tokens=512,\n",
        "              min_new_tokens=-1,\n",
        "              top_k=30\n",
        "\n",
        "              )"
      ],
      "metadata": {
        "id": "7jvzHFEEQzHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "i-ZHxO1WRcxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "G9vCMg5ryqA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "k9GHRzLGpy8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd950aca-ef31-46cc-815e-5d1e2363d94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7be7d2380640>, model_id='gpt2', model_kwargs={'temperature': 0}, pipeline_kwargs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 10: Creating a memory object which is necessary to track inputs/outputs and hold a conversation**"
      ],
      "metadata": {
        "id": "5nyLvxUCSf6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
      ],
      "metadata": {
        "id": "iBUaCuuCRmST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 11: Creating a Conversation Retrieval QA Chain**"
      ],
      "metadata": {
        "id": "vaaKQ07wTJZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.\n",
        "\n"
      ],
      "metadata": {
        "id": "2iBRTgn0TOn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create our Q/A Chain\n",
        "pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                             retriever=vectordb.as_retriever(search_kwargs={'k':6}),\n",
        "                                             verbose=False, memory=memory)"
      ],
      "metadata": {
        "id": "9rHVF7DiTB5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=pdf_qa({\"question\":\"YOLOv7 is trained on which dataset\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "oiZRnvtuTlXw",
        "outputId": "ad774709-393a-4eef-80c9-9a1d4ce55b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-aa65fe1b5a02>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"YOLOv7 is trained on which dataset\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             outputs = (\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mnew_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mnew_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"chat_history\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_history_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         answer = self.combine_docs_chain.run(\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             outputs = (\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# Call predict on the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     async def acombine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \"\"\"\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             outputs = (\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    450\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             ]\n\u001b[0;32m--> 582\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             output = (\n\u001b[0;32m--> 475\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    476\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             text = (\n\u001b[0;32m--> 961\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     ) -> str:\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# Text generation return includes the starter text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1539\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 )\n\u001b[1;32m    692\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    694\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m# upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.48 GiB (GPU 0; 14.75 GiB total capacity; 11.79 GiB already allocated; 1.51 GiB free; 12.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "h4pzVFkxLU36",
        "outputId": "ffca4a01-63cd-4d4a-f7a2-d08116ca58bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-542d9e7f56a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------------------------------------------------------------------------------')\n",
        "print('Welcome to the DocBot. You are now ready to start interacting with your documents')\n",
        "print('---------------------------------------------------------------------------------')\n",
        "\n",
        "while True:\n",
        "  query=input(f\"Prompt:\")\n",
        "  if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if query == '':\n",
        "    continue\n",
        "  result = pdf_qa({\"question\": query})\n",
        "  print(f\"Answer: \" + result[\"answer\"])\n"
      ],
      "metadata": {
        "id": "P3SmOH3aV_bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAgc6G-DVvuG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}