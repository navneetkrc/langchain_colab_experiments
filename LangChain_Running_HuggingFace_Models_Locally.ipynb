{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/langchain_colab_experiments/blob/main/LangChain_Running_HuggingFace_Models_Locally.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this Video by Sam Witteveen\n",
        "https://www.youtube.com/watch?v=Kn7SX2Mx_Jk\n",
        "Colab Code Notebook: [https://drp.li/m1mbM](https://drp.li/m1mbM)"
      ],
      "metadata": {
        "id": "GJc0JBHbASzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local LLM Models \n",
        "\n",
        "* google/flan-t5-xl\n",
        "* facebook/blenderbot-1B-distill\n",
        "* sentence-transformers/all-mpnet-base-v2,\n",
        "* gpt2-medium\n",
        "\n"
      ],
      "metadata": {
        "id": "92wQbt1fDw6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i9fJuC2tG0Cq"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub transformers sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace\n",
        "\n",
        "There are two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: text2text-generation, text-generation\n"
      ],
      "metadata": {
        "id": "VkVTT54xNq8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''"
      ],
      "metadata": {
        "id": "2PpqSc3cG7jJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the HuggingFaceHub"
      ],
      "metadata": {
        "id": "UhauDrynY0cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "Derb_0t-ZESh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", \n",
        "                                        model_kwargs={\"temperature\":0, \n",
        "                                                      \"max_length\":64}))"
      ],
      "metadata": {
        "id": "lvO31GCGKhHs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of France?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWIc38V4t5gA",
        "outputId": "b375e49b-2686-4caf-e351-f627384eafbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is the capital of France. The final answer: Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What area is best for growing wine in France?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK61DssCdCmr",
        "outputId": "fbcb1af1-2f40-4f89-9d3c-97cc2af3ad5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best area for growing wine in France is the Loire Valley. The Loire Valley is located in the south of France. The area of France that is best for growing wine is the Loire Valley. The final answer: Loire Valley.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BlenderBot\n",
        "\n",
        "Doesn't work on the Hub"
      ],
      "metadata": {
        "id": "bsg2rRuNwNb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "blenderbot_chain = LLMChain(prompt=prompt, \n",
        "                     llm=HuggingFaceHub(repo_id=\"facebook/blenderbot-1B-distill\", \n",
        "                                        model_kwargs={\"temperature\":0, \n",
        "                                                      \"max_length\":64}))\n",
        "                                                      '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Io-4j_aat_Ll",
        "outputId": "afe1f8c0-4a3c-4523-a31f-bacbeff25dc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nblenderbot_chain = LLMChain(prompt=prompt, \\n                     llm=HuggingFaceHub(repo_id=\"facebook/blenderbot-1B-distill\", \\n                                        model_kwargs={\"temperature\":0, \\n                                                      \"max_length\":64}))\\n                                                      '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"What is the capital of France?\"\n",
        "# question = \"What area is best for growing wine in France?\"\n",
        "\n",
        "# print(blenderbot_chain = LLMChain(prompt=prompt, \n",
        "# .run(question))"
      ],
      "metadata": {
        "id": "F8hXzlWZuOhE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Local model from HF \n",
        "\n",
        "### Why would you want to use local mode?\n",
        "\n",
        "- fine-tuned models\n",
        "- GPU hosted etc\n",
        "- some models only work locally"
      ],
      "metadata": {
        "id": "-xUmcqkUf9Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5-Flan - Encoder-Decoder"
      ],
      "metadata": {
        "id": "WXOZ_Un6e1oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=100\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)\n"
      ],
      "metadata": {
        "id": "GMg2xiRnfm21"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(local_llm('What is the capital of France? '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvsLFQxehfZe",
        "outputId": "d7f9b5c8-59fe-4142-9ff4-e63c0b17d046"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=local_llm\n",
        "                     )\n",
        "\n",
        "question = \"What is the capital of England?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UVVbwLegmU8",
        "outputId": "9ccc215a-561a-4f59-e105-515e67efd968"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of England is London. London is the capital of England. So the answer is London.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT2-medium - Decoder Only Model\n",
        "\n",
        "microsoft/DialoGPT-large"
      ],
      "metadata": {
        "id": "ccAHJARfnKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", \n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=100\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "wEKTalGcgxRg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=local_llm\n",
        "                     )\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNMzu_bznhBp",
        "outputId": "f790fd3e-e287-41d8-c2bc-7c87dd09bb8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Capital of Paris – France\n",
            "\n",
            "Source: Wikipedia\n",
            "\n",
            "2. Capital of Paris – Switzerland\n",
            "\n",
            "Source: Wikipedia\n",
            "\n",
            "3. Capital of Paris – Luxembourg\n",
            "\n",
            "Source: Wikipedia\n",
            "\n",
            "4. Capital of Paris – Netherlands\n",
            "\n",
            "Source: Wikipedia\n",
            "\n",
            "5. Capital of Paris – Germany\n",
            "\n",
            "Source: Wikipedia\n",
            "\n",
            "6. Capital of Paris – United Kingdom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BlenderBot - Encoder-Decoder"
      ],
      "metadata": {
        "id": "UHikkKC0u3dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = 'facebook/blenderbot-1B-distill'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=100\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "45AQKsOfu5WN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=local_llm\n",
        "                     )\n",
        "\n",
        "question = \"What area is best for growing wine in France?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpNLf2lovgO7",
        "outputId": "0f4f705b-6a45-45ce-ffa5-943622d27e0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'm not sure, but I do know that France is one of the largest producers of wine in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentenceTransformers"
      ],
      "metadata": {
        "id": "z66tHc3V5yv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "hf = HuggingFaceEmbeddings(model_name=model_name)"
      ],
      "metadata": {
        "id": "aKeSAVQUn8fN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.embed_query('this is an embedding')"
      ],
      "metadata": {
        "id": "ZORCrZkaykTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.embed_documents(['this is an embedding','this another embedding'])"
      ],
      "metadata": {
        "id": "-weIzgnSys2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "hf = HuggingFaceHubEmbeddings(\n",
        "    repo_id=model_name,\n",
        "    task=\"feature-extraction\",\n",
        "    # huggingfacehub_api_token=\"my-api-key\",\n",
        ")"
      ],
      "metadata": {
        "id": "KBwmKrk4smhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pR62_JRxARbA"
      }
    }
  ]
}